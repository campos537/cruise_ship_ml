{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "headed-center",
   "metadata": {},
   "source": [
    "## Crew Number PyTorch\n",
    "\n",
    "### This notebook shows about training an model to predict the Crew number step by step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-solution",
   "metadata": {},
   "source": [
    "### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "atlantic-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import visuals as vs\n",
    "from torch import optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-council",
   "metadata": {},
   "source": [
    "### 2. Load the data and show preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"cruise_ship_info.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neutral-conditions",
   "metadata": {},
   "source": [
    "### 3.  Separe the crew (that we want predict) from the features and get the numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "august-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew = data['crew']\n",
    "features = data.drop('crew', axis = 1)\n",
    "features.shape, crew.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-camera",
   "metadata": {},
   "source": [
    "### 4. Show basic statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistics for Cruise Ship dataset:\\n\")\n",
    "print(\"Minimum crew: {}\".format(crew.min())) \n",
    "print(\"Maximum crew: {}\".format(crew.max()))\n",
    "print(\"Mean crew: {}\".format(crew.mean()))\n",
    "print(\"Median crew {}\".format(np.median(crew)))\n",
    "print(\"Standard deviation of crew value: {}\".format(crew.std()))\n",
    "print(\"Total of crew values: {}\".format(crew.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-sociology",
   "metadata": {},
   "source": [
    "We can see that the difference between the minimum and maximum crew size are large so we can infer that we are working with really small ships and with really large ones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-isaac",
   "metadata": {},
   "source": [
    "### 5. Choose the best features for a crew predictor\n",
    "At first for sure i would remove the **Ship_name** since each ship has one so it isn't useful, and intuitivelly remove **Age** and **Cruise_line** but i rather check which is the feature importance. The features **Tonnage**, **passengers**, **length**, **passenger_density** and **cabins** in my point of view are all importants since a crew value should be choosen depending on the demand of those features (Higher the features value also would be the crew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Ship_name column from the input data \n",
    "data_clean = features.drop('Ship_name', axis = 1)\n",
    "data_clean[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-jerusalem",
   "metadata": {},
   "source": [
    "#### 5.1. One Hot Encode the Cruise_line feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vital-titanium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one hot encoding of columns Cruise_line\n",
    "one_hot = pd.get_dummies(data_clean['Cruise_line'])\n",
    "# Drop column Cruise_line as it is now encoded\n",
    "data_clean = data_clean.drop('Cruise_line',axis = 1)\n",
    "# Join the encoded df\n",
    "data_clean = data_clean.join(one_hot)\n",
    "print(data_clean.shape)\n",
    "data_clean[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-blackjack",
   "metadata": {},
   "source": [
    "#### 5.3 Check the feature importance without the Cruise_line and with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-perry",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "X = data_clean.values\n",
    "y = crew.values\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = lr.coef_\n",
    "# summarize feature importance\n",
    "vs.feature_plot(importance,data_clean,crew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-mandate",
   "metadata": {},
   "source": [
    "#### 5.4 Drop columns with less important features\n",
    "After checking the importance of the features will drop a the columns that aren't useful for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "allied-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importance)[::-1]\n",
    "useful_features = len([v for v in importance if v > 0])\n",
    "print(\"Less important features: \", data_clean.columns.values[indices[useful_features:]])\n",
    "data_clean = data_clean.drop(data_clean.columns.values[indices[useful_features:]], axis=1)\n",
    "X = data_clean.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-container",
   "metadata": {},
   "source": [
    "### 6. Convert the data to PyTorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 95\n",
    "valid_len = 63\n",
    "\n",
    "# Convert dataset to PyTorch\n",
    "dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "train_ds, val_ds = random_split(dataset, [train_len, valid_len])\n",
    "train_loader = DataLoader(train_ds, 1)\n",
    "val_loader = DataLoader(val_ds, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-newman",
   "metadata": {},
   "source": [
    "### 7. Create the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        \n",
    "    def forward(self, input_): \n",
    "        out = self.linear(input_)\n",
    "        return out\n",
    "model = ShipModel(data_clean.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-kennedy",
   "metadata": {},
   "source": [
    "### 8. Set the main hyper-parameters\n",
    "the hyper-parameters need to be used to make the model to converge, if you choose a higher learning rater for example the model would overfit and if you choose a really small learning rate it will underfit, the same for the quantity of epochs you choose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-rates",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Square Error was choosen since its a great loss for a Regression problem \n",
    "criterion = nn.MSELoss()\n",
    "# The Learning rate was tuned to find the best one to fit the data without causing overfitting or underfitting\n",
    "learning_rate=0.000004\n",
    "# SGD Was used since ts a great Optimizer used for general purposes\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# Epochs\n",
    "epochs = 25\n",
    "# Assign to the device\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-vietnamese",
   "metadata": {},
   "source": [
    "### 9. Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-trade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, testloader, epochs, criterion, optmizer, model_name):\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_every = 1\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, values in trainloader:\n",
    "            steps += 1\n",
    "            inputs, values = inputs.to(\"cpu\"), values.to(\"cpu\")\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, values)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            if steps % print_every == 0:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, values in testloader:\n",
    "                        inputs, values = inputs.to(\"cpu\"), values.to(\"cpu\")\n",
    "                        logps = model.forward(inputs)\n",
    "                        batch_loss = criterion(logps, values)\n",
    "                        test_loss += batch_loss.item()\n",
    "\n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                val_loss_fix = round(test_loss/len(testloader),3)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                       f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                       f\"Validation loss: {val_loss_fix}.. \")\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "    #       Save every epoch\n",
    "            if not os.path.exists('models/'+model_name+'/'):\n",
    "                os.mkdir('models/'+model_name+'/')\n",
    "            model_full_name = 'models/'+model_name+'/epoch_'+str(epoch)+'_name_'+model_name+'_.pt'\n",
    "            torch.save(model, model_full_name) # official recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recreational-bonus",
   "metadata": {},
   "source": [
    "### 10. Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model ,train_loader, val_loader, epochs, criterion, optimizer, \"model_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-fraction",
   "metadata": {},
   "source": [
    "### 11. Show some predictions and compare with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-vacuum",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"ground-truth: \",float(val_ds[i][1]), \" Predicted: \",float(model(val_ds[i][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-refund",
   "metadata": {},
   "source": [
    "### 12. Measure Pearson Corrrelation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-machinery",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = val_ds.dataset.tensors[1].cpu().detach().numpy()\n",
    "\n",
    "pred_train = np.ones(95, dtype=np.float32)\n",
    "y_train = np.ones(95, dtype=np.float32)\n",
    "\n",
    "pred_val = np.ones(63, dtype=np.float32)\n",
    "y_val = np.ones(63, dtype=np.float32)\n",
    "\n",
    "count = 0\n",
    "for point in train_ds:\n",
    "    pred_train[count] = float(model(point[0]))\n",
    "    y_train[count] = float(point[1])\n",
    "    count+=1\n",
    "    \n",
    "count = 0\n",
    "for point in val_ds:\n",
    "    pred_val[count] = float(model(point[0]))\n",
    "    y_val[count] = float(point[1])\n",
    "    count+=1\n",
    "\n",
    "corr_train = np.corrcoef(y_train,pred_train, np.float64)\n",
    "print(corr_train)\n",
    "\n",
    "corr_val = np.corrcoef(y_val,pred_val, np.float64)\n",
    "print(corr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-compound",
   "metadata": {},
   "source": [
    "The model seems to have a great value of Pearson Correlation Coefficient getting close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-justice",
   "metadata": {},
   "source": [
    "### 13. Regularization\n",
    "Regularization is used in order to avoid overfitting, for that the SGD Optimizer aproach already deals with L2 Regularization which is the same as this model trained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cruise_env",
   "language": "python",
   "name": "cruise_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
