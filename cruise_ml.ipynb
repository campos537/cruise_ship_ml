{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supported-greece",
   "metadata": {},
   "source": [
    "## Crew Number PyTorch\n",
    "\n",
    "### This notebook shows about training an model to predict the Crew number step by step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-scheme",
   "metadata": {},
   "source": [
    "### 1. Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ambient-stereo",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import visuals as vs\n",
    "from torch import optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-landing",
   "metadata": {},
   "source": [
    "### 2. Load the data and show preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cultural-somerset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ship_name</th>\n",
       "      <th>Cruise_line</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tonnage</th>\n",
       "      <th>passengers</th>\n",
       "      <th>length</th>\n",
       "      <th>cabins</th>\n",
       "      <th>passenger_density</th>\n",
       "      <th>crew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Journey</td>\n",
       "      <td>Azamara</td>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quest</td>\n",
       "      <td>Azamara</td>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Celebration</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>26</td>\n",
       "      <td>47.262</td>\n",
       "      <td>14.86</td>\n",
       "      <td>7.22</td>\n",
       "      <td>7.43</td>\n",
       "      <td>31.80</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Conquest</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>11</td>\n",
       "      <td>110.000</td>\n",
       "      <td>29.74</td>\n",
       "      <td>9.53</td>\n",
       "      <td>14.88</td>\n",
       "      <td>36.99</td>\n",
       "      <td>19.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Destiny</td>\n",
       "      <td>Carnival</td>\n",
       "      <td>17</td>\n",
       "      <td>101.353</td>\n",
       "      <td>26.42</td>\n",
       "      <td>8.92</td>\n",
       "      <td>13.21</td>\n",
       "      <td>38.36</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Taurus</td>\n",
       "      <td>Star</td>\n",
       "      <td>22</td>\n",
       "      <td>3.341</td>\n",
       "      <td>0.66</td>\n",
       "      <td>2.79</td>\n",
       "      <td>0.33</td>\n",
       "      <td>50.62</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Virgo</td>\n",
       "      <td>Star</td>\n",
       "      <td>14</td>\n",
       "      <td>76.800</td>\n",
       "      <td>19.60</td>\n",
       "      <td>8.79</td>\n",
       "      <td>9.67</td>\n",
       "      <td>39.18</td>\n",
       "      <td>12.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Spirit</td>\n",
       "      <td>Windstar</td>\n",
       "      <td>25</td>\n",
       "      <td>5.350</td>\n",
       "      <td>1.58</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.74</td>\n",
       "      <td>33.86</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Star</td>\n",
       "      <td>Windstar</td>\n",
       "      <td>27</td>\n",
       "      <td>5.350</td>\n",
       "      <td>1.67</td>\n",
       "      <td>4.40</td>\n",
       "      <td>0.74</td>\n",
       "      <td>32.04</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Surf</td>\n",
       "      <td>Windstar</td>\n",
       "      <td>23</td>\n",
       "      <td>14.745</td>\n",
       "      <td>3.08</td>\n",
       "      <td>6.17</td>\n",
       "      <td>1.56</td>\n",
       "      <td>47.87</td>\n",
       "      <td>1.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>158 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Ship_name Cruise_line  Age  Tonnage  passengers  length  cabins  \\\n",
       "0        Journey     Azamara    6   30.277        6.94    5.94    3.55   \n",
       "1          Quest     Azamara    6   30.277        6.94    5.94    3.55   \n",
       "2    Celebration    Carnival   26   47.262       14.86    7.22    7.43   \n",
       "3       Conquest    Carnival   11  110.000       29.74    9.53   14.88   \n",
       "4        Destiny    Carnival   17  101.353       26.42    8.92   13.21   \n",
       "..           ...         ...  ...      ...         ...     ...     ...   \n",
       "153       Taurus        Star   22    3.341        0.66    2.79    0.33   \n",
       "154        Virgo        Star   14   76.800       19.60    8.79    9.67   \n",
       "155       Spirit    Windstar   25    5.350        1.58    4.40    0.74   \n",
       "156         Star    Windstar   27    5.350        1.67    4.40    0.74   \n",
       "157         Surf    Windstar   23   14.745        3.08    6.17    1.56   \n",
       "\n",
       "     passenger_density   crew  \n",
       "0                42.64   3.55  \n",
       "1                42.64   3.55  \n",
       "2                31.80   6.70  \n",
       "3                36.99  19.10  \n",
       "4                38.36  10.00  \n",
       "..                 ...    ...  \n",
       "153              50.62   0.59  \n",
       "154              39.18  12.00  \n",
       "155              33.86   0.88  \n",
       "156              32.04   0.88  \n",
       "157              47.87   1.80  \n",
       "\n",
       "[158 rows x 9 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"cruise_ship_info.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-train",
   "metadata": {},
   "source": [
    "### 3.  Separe the crew (that we want predict) from the features and get the numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "clear-cross",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((158, 8), (158,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crew = data['crew']\n",
    "features = data.drop('crew', axis = 1)\n",
    "features.shape, crew.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-election",
   "metadata": {},
   "source": [
    "### 4. Show basic statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "billion-closer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Cruise Ship dataset:\n",
      "\n",
      "Minimum crew: 0.59\n",
      "Maximum crew: 21.0\n",
      "Mean crew: 7.794177215189873\n",
      "Median crew 8.15\n",
      "Standard deviation of crew value: 3.503486564627033\n",
      "Total of crew values: 158\n"
     ]
    }
   ],
   "source": [
    "print(\"Statistics for Cruise Ship dataset:\\n\")\n",
    "print(\"Minimum crew: {}\".format(crew.min())) \n",
    "print(\"Maximum crew: {}\".format(crew.max()))\n",
    "print(\"Mean crew: {}\".format(crew.mean()))\n",
    "print(\"Median crew {}\".format(np.median(crew)))\n",
    "print(\"Standard deviation of crew value: {}\".format(crew.std()))\n",
    "print(\"Total of crew values: {}\".format(crew.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-curve",
   "metadata": {},
   "source": [
    "We can see that the difference between the minimum and maximum crew size are large so we can infer that we are working with really small ships and with really large ones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-sampling",
   "metadata": {},
   "source": [
    "### 5. Choose the best features for a crew predictor\n",
    "At first for sure i would remove the **Ship_name** since each ship has one so it isn't useful, and intuitivelly remove **Age** and **Cruise_line** but i rather check which is the feature importance. The features **Tonnage**, **passengers**, **length**, **passenger_density** and **cabins** in my point of view are all importants since a crew value should be choosen depending on the demand of those features (Higher the features value also would be the crew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "turned-replica",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cruise_line</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tonnage</th>\n",
       "      <th>passengers</th>\n",
       "      <th>length</th>\n",
       "      <th>cabins</th>\n",
       "      <th>passenger_density</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Azamara</td>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azamara</td>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carnival</td>\n",
       "      <td>26</td>\n",
       "      <td>47.262</td>\n",
       "      <td>14.86</td>\n",
       "      <td>7.22</td>\n",
       "      <td>7.43</td>\n",
       "      <td>31.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Cruise_line  Age  Tonnage  passengers  length  cabins  passenger_density\n",
       "0     Azamara    6   30.277        6.94    5.94    3.55              42.64\n",
       "1     Azamara    6   30.277        6.94    5.94    3.55              42.64\n",
       "2    Carnival   26   47.262       14.86    7.22    7.43              31.80"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete the Ship_name column from the input data \n",
    "data_clean = features.drop('Ship_name', axis = 1)\n",
    "data_clean[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-assumption",
   "metadata": {},
   "source": [
    "#### 5.1. One Hot Encode the Cruise_line feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fuzzy-webster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(158, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Tonnage</th>\n",
       "      <th>passengers</th>\n",
       "      <th>length</th>\n",
       "      <th>cabins</th>\n",
       "      <th>passenger_density</th>\n",
       "      <th>Azamara</th>\n",
       "      <th>Carnival</th>\n",
       "      <th>Celebrity</th>\n",
       "      <th>Costa</th>\n",
       "      <th>...</th>\n",
       "      <th>Oceania</th>\n",
       "      <th>Orient</th>\n",
       "      <th>P&amp;O</th>\n",
       "      <th>Princess</th>\n",
       "      <th>Regent_Seven_Seas</th>\n",
       "      <th>Royal_Caribbean</th>\n",
       "      <th>Seabourn</th>\n",
       "      <th>Silversea</th>\n",
       "      <th>Star</th>\n",
       "      <th>Windstar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>30.277</td>\n",
       "      <td>6.94</td>\n",
       "      <td>5.94</td>\n",
       "      <td>3.55</td>\n",
       "      <td>42.64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>47.262</td>\n",
       "      <td>14.86</td>\n",
       "      <td>7.22</td>\n",
       "      <td>7.43</td>\n",
       "      <td>31.80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Tonnage  passengers  length  cabins  passenger_density  Azamara  \\\n",
       "0    6   30.277        6.94    5.94    3.55              42.64        1   \n",
       "1    6   30.277        6.94    5.94    3.55              42.64        1   \n",
       "2   26   47.262       14.86    7.22    7.43              31.80        0   \n",
       "\n",
       "   Carnival  Celebrity  Costa  ...  Oceania  Orient  P&O  Princess  \\\n",
       "0         0          0      0  ...        0       0    0         0   \n",
       "1         0          0      0  ...        0       0    0         0   \n",
       "2         1          0      0  ...        0       0    0         0   \n",
       "\n",
       "   Regent_Seven_Seas  Royal_Caribbean  Seabourn  Silversea  Star  Windstar  \n",
       "0                  0                0         0          0     0         0  \n",
       "1                  0                0         0          0     0         0  \n",
       "2                  0                0         0          0     0         0  \n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get one hot encoding of columns Cruise_line\n",
    "one_hot = pd.get_dummies(data_clean['Cruise_line'])\n",
    "# Drop column Cruise_line as it is now encoded\n",
    "data_clean = data_clean.drop('Cruise_line',axis = 1)\n",
    "# Join the encoded df\n",
    "data_clean = data_clean.join(one_hot)\n",
    "print(data_clean.shape)\n",
    "data_clean[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-longitude",
   "metadata": {},
   "source": [
    "#### 5.3 Check the feature importance without the Cruise_line and with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "played-table",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from matplotlib import pyplot\n",
    "\n",
    "X = data_clean.values\n",
    "y = crew.values\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chinese-subscriber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "importance = lr.coef_\n",
    "# summarize feature importance\n",
    "vs.feature_plot(importance,data_clean,crew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-builder",
   "metadata": {},
   "source": [
    "#### 5.4 Drop columns with less important features\n",
    "After checking the importance of the features will drop a the columns that aren't useful for our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "passive-country",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.29862723e+00 -6.47536545e-01 -5.22423960e-01 -5.15718963e-01\n",
      " -3.51419944e-01 -2.20675165e-01 -1.81687618e-01 -1.48694789e-01\n",
      " -9.79161172e-02 -8.00197332e-02  7.39850930e-04  2.23069281e-03\n",
      "  7.27089135e-03  1.45775342e-02  8.19339183e-02  1.10020048e-01\n",
      "  1.51424518e-01  2.08264661e-01  3.03465301e-01  4.08654152e-01\n",
      "  4.74881573e-01  4.78986876e-01  4.85078106e-01  5.35711740e-01\n",
      "  7.24860091e-01  1.21123046e+00]\n",
      "Less important features:  ['Cunard' 'passengers' 'Princess' 'P&O' 'MSC' 'Windstar' 'Costa' 'Orient'\n",
      " 'Holland_American' 'Royal_Caribbean']\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(importance)[::-1]\n",
    "print(np.sort(importance))\n",
    "print(\"Less important features: \", data_clean.columns.values[indices[16:]])\n",
    "data_clean = data_clean.drop(data_clean.columns.values[indices[16:]], axis=1)\n",
    "X = data_clean.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-competition",
   "metadata": {},
   "source": [
    "### 6. Convert the data to PyTorch format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "integrated-sister",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = 95\n",
    "valid_len = 63\n",
    "\n",
    "# Convert dataset to PyTorch\n",
    "dataset = TensorDataset(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "train_ds, val_ds = random_split(dataset, [train_len, valid_len])\n",
    "train_loader = DataLoader(train_ds, 1)\n",
    "val_loader = DataLoader(val_ds, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-forty",
   "metadata": {},
   "source": [
    "### 7. Create the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "appropriate-claim",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShipModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "        \n",
    "    def forward(self, input_): \n",
    "        out = self.linear(input_)\n",
    "        return out\n",
    "model = ShipModel(data_clean.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "healthy-liability",
   "metadata": {},
   "source": [
    "### 8. Set the main hyper-parameters\n",
    "the hyper-parameters need to be used to make the model to converge, if you choose a higher learning rater for example the model would overfit and if you choose a really small learning rate it will underfit, the same for the quantity of epochs you choose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "northern-bradford",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShipModel(\n",
       "  (linear): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mean Square Error was choosen since its a great loss for a Regression problem \n",
    "criterion = nn.MSELoss()\n",
    "# The Learning rate was tuned to find the best one to fit the data without causing overfitting or underfitting\n",
    "learning_rate=0.000004\n",
    "# SGD Was used since ts a great Optimizer used for general purposes\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# Epochs\n",
    "epochs = 20\n",
    "# Assign to the device\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-cheese",
   "metadata": {},
   "source": [
    "### 9. Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dominican-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, testloader, epochs, criterion, optmizer, model_name):\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_every = 1\n",
    "    train_losses, test_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, values in trainloader:\n",
    "            steps += 1\n",
    "            inputs, values = inputs.to(\"cpu\"), values.to(\"cpu\")\n",
    "            logps = model.forward(inputs)\n",
    "            loss = criterion(logps, values)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "            if steps % print_every == 0:\n",
    "                test_loss = 0\n",
    "                accuracy = 0\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    for inputs, values in testloader:\n",
    "                        inputs, values = inputs.to(\"cpu\"), values.to(\"cpu\")\n",
    "                        logps = model.forward(inputs)\n",
    "                        batch_loss = criterion(logps, values)\n",
    "                        test_loss += batch_loss.item()\n",
    "\n",
    "                train_losses.append(running_loss/len(trainloader))\n",
    "                test_losses.append(test_loss/len(testloader))\n",
    "\n",
    "                val_loss_fix = round(test_loss/len(testloader),3)\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
    "                       f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
    "                       f\"Validation loss: {val_loss_fix}.. \")\n",
    "                running_loss = 0\n",
    "                model.train()\n",
    "    #       Save every epoch\n",
    "            if not os.path.exists('models/'+model_name+'/'):\n",
    "                os.mkdir('models/'+model_name+'/')\n",
    "            model_full_name = 'models/'+model_name+'/epoch_'+str(epoch)+'_name_'+model_name+'_.pt'\n",
    "            torch.save(model, model_full_name) # official recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-touch",
   "metadata": {},
   "source": [
    "### 10. Train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "veterinary-private",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20.. Train loss: 0.856.. Validation loss: 26.026.. \n",
      "Epoch 1/20.. Train loss: 2.573.. Validation loss: 26.577.. \n",
      "Epoch 1/20.. Train loss: 2.556.. Validation loss: 25.986.. \n",
      "Epoch 1/20.. Train loss: 9.035.. Validation loss: 24.119.. \n",
      "Epoch 1/20.. Train loss: 4.824.. Validation loss: 22.88.. \n",
      "Epoch 1/20.. Train loss: 4.524.. Validation loss: 21.719.. \n",
      "Epoch 1/20.. Train loss: 6.148.. Validation loss: 20.543.. \n",
      "Epoch 1/20.. Train loss: 5.958.. Validation loss: 20.886.. \n",
      "Epoch 1/20.. Train loss: 9.089.. Validation loss: 19.568.. \n",
      "Epoch 1/20.. Train loss: 1.317.. Validation loss: 19.669.. \n",
      "Epoch 1/20.. Train loss: 0.069.. Validation loss: 19.608.. \n",
      "Epoch 1/20.. Train loss: 0.205.. Validation loss: 19.487.. \n",
      "Epoch 1/20.. Train loss: 2.540.. Validation loss: 19.575.. \n",
      "Epoch 1/20.. Train loss: 33.075.. Validation loss: 16.487.. \n",
      "Epoch 1/20.. Train loss: 9.333.. Validation loss: 16.753.. \n",
      "Epoch 1/20.. Train loss: 1.801.. Validation loss: 16.336.. \n",
      "Epoch 1/20.. Train loss: 8.595.. Validation loss: 16.789.. \n",
      "Epoch 1/20.. Train loss: 4.721.. Validation loss: 15.783.. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/crystal/main_files/Repositorios/Research-tools-study/ml_task/cruise_ship_ml/cruise_env/lib/python3.7/site-packages/torch/nn/modules/loss.py:431: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/media/crystal/main_files/Repositorios/Research-tools-study/ml_task/cruise_ship_ml/cruise_env/lib/python3.7/site-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type ShipModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20.. Train loss: 22.247.. Validation loss: 13.465.. \n",
      "Epoch 1/20.. Train loss: 0.320.. Validation loss: 13.326.. \n",
      "Epoch 1/20.. Train loss: 0.988.. Validation loss: 12.995.. \n",
      "Epoch 1/20.. Train loss: 0.317.. Validation loss: 12.855.. \n",
      "Epoch 1/20.. Train loss: 0.651.. Validation loss: 12.747.. \n",
      "Epoch 1/20.. Train loss: 0.619.. Validation loss: 12.498.. \n",
      "Epoch 1/20.. Train loss: 2.812.. Validation loss: 12.801.. \n",
      "Epoch 1/20.. Train loss: 92.244.. Validation loss: 8.154.. \n",
      "Epoch 1/20.. Train loss: 0.285.. Validation loss: 8.252.. \n",
      "Epoch 1/20.. Train loss: 0.817.. Validation loss: 8.07.. \n",
      "Epoch 1/20.. Train loss: 0.308.. Validation loss: 8.161.. \n",
      "Epoch 1/20.. Train loss: 9.228.. Validation loss: 7.419.. \n",
      "Epoch 1/20.. Train loss: 3.339.. Validation loss: 7.172.. \n",
      "Epoch 1/20.. Train loss: 1.149.. Validation loss: 7.237.. \n",
      "Epoch 1/20.. Train loss: 8.036.. Validation loss: 7.407.. \n",
      "Epoch 1/20.. Train loss: 7.056.. Validation loss: 7.583.. \n",
      "Epoch 1/20.. Train loss: 5.409.. Validation loss: 7.751.. \n",
      "Epoch 1/20.. Train loss: 4.915.. Validation loss: 7.331.. \n",
      "Epoch 1/20.. Train loss: 6.826.. Validation loss: 7.508.. \n",
      "Epoch 1/20.. Train loss: 5.954.. Validation loss: 7.005.. \n",
      "Epoch 1/20.. Train loss: 8.227.. Validation loss: 7.111.. \n",
      "Epoch 1/20.. Train loss: 19.585.. Validation loss: 6.103.. \n",
      "Epoch 1/20.. Train loss: 0.059.. Validation loss: 6.079.. \n",
      "Epoch 1/20.. Train loss: 1.935.. Validation loss: 5.953.. \n",
      "Epoch 1/20.. Train loss: 3.692.. Validation loss: 5.767.. \n",
      "Epoch 1/20.. Train loss: 0.007.. Validation loss: 5.767.. \n",
      "Epoch 1/20.. Train loss: 0.441.. Validation loss: 5.804.. \n",
      "Epoch 1/20.. Train loss: 0.354.. Validation loss: 5.823.. \n",
      "Epoch 1/20.. Train loss: 1.881.. Validation loss: 5.883.. \n",
      "Epoch 1/20.. Train loss: 0.006.. Validation loss: 5.876.. \n",
      "Epoch 1/20.. Train loss: 0.016.. Validation loss: 5.866.. \n",
      "Epoch 1/20.. Train loss: 1.859.. Validation loss: 5.75.. \n",
      "Epoch 1/20.. Train loss: 13.286.. Validation loss: 5.844.. \n",
      "Epoch 1/20.. Train loss: 6.116.. Validation loss: 5.964.. \n",
      "Epoch 1/20.. Train loss: 2.596.. Validation loss: 5.804.. \n",
      "Epoch 1/20.. Train loss: 23.716.. Validation loss: 6.256.. \n",
      "Epoch 1/20.. Train loss: 5.287.. Validation loss: 5.901.. \n",
      "Epoch 1/20.. Train loss: 1.244.. Validation loss: 5.994.. \n",
      "Epoch 1/20.. Train loss: 0.474.. Validation loss: 5.932.. \n",
      "Epoch 1/20.. Train loss: 0.655.. Validation loss: 6.008.. \n",
      "Epoch 1/20.. Train loss: 0.932.. Validation loss: 6.123.. \n",
      "Epoch 1/20.. Train loss: 0.646.. Validation loss: 6.013.. \n",
      "Epoch 1/20.. Train loss: 0.802.. Validation loss: 6.073.. \n",
      "Epoch 1/20.. Train loss: 3.071.. Validation loss: 6.139.. \n",
      "Epoch 1/20.. Train loss: 0.257.. Validation loss: 6.089.. \n",
      "Epoch 1/20.. Train loss: 0.368.. Validation loss: 6.025.. \n",
      "Epoch 1/20.. Train loss: 23.740.. Validation loss: 5.377.. \n",
      "Epoch 1/20.. Train loss: 0.561.. Validation loss: 5.35.. \n",
      "Epoch 1/20.. Train loss: 2.982.. Validation loss: 5.399.. \n",
      "Epoch 1/20.. Train loss: 0.299.. Validation loss: 5.428.. \n",
      "Epoch 1/20.. Train loss: 0.000.. Validation loss: 5.428.. \n",
      "Epoch 1/20.. Train loss: 20.265.. Validation loss: 5.446.. \n",
      "Epoch 1/20.. Train loss: 1.712.. Validation loss: 5.563.. \n",
      "Epoch 1/20.. Train loss: 0.231.. Validation loss: 5.604.. \n",
      "Epoch 1/20.. Train loss: 1.276.. Validation loss: 5.696.. \n",
      "Epoch 1/20.. Train loss: 0.220.. Validation loss: 5.634.. \n",
      "Epoch 1/20.. Train loss: 0.985.. Validation loss: 5.717.. \n",
      "Epoch 1/20.. Train loss: 0.345.. Validation loss: 5.786.. \n",
      "Epoch 1/20.. Train loss: 0.002.. Validation loss: 5.781.. \n",
      "Epoch 1/20.. Train loss: 10.544.. Validation loss: 5.861.. \n",
      "Epoch 1/20.. Train loss: 1.055.. Validation loss: 5.989.. \n",
      "Epoch 1/20.. Train loss: 0.660.. Validation loss: 5.849.. \n",
      "Epoch 1/20.. Train loss: 0.217.. Validation loss: 5.806.. \n",
      "Epoch 1/20.. Train loss: 1.526.. Validation loss: 5.914.. \n",
      "Epoch 1/20.. Train loss: 0.585.. Validation loss: 5.786.. \n",
      "Epoch 1/20.. Train loss: 0.109.. Validation loss: 5.807.. \n",
      "Epoch 1/20.. Train loss: 0.008.. Validation loss: 5.813.. \n",
      "Epoch 1/20.. Train loss: 32.778.. Validation loss: 6.965.. \n",
      "Epoch 1/20.. Train loss: 1.405.. Validation loss: 6.745.. \n",
      "Epoch 1/20.. Train loss: 0.061.. Validation loss: 6.785.. \n",
      "Epoch 1/20.. Train loss: 13.301.. Validation loss: 6.029.. \n",
      "Epoch 1/20.. Train loss: 0.031.. Validation loss: 6.025.. \n",
      "Epoch 1/20.. Train loss: 1.010.. Validation loss: 6.144.. \n",
      "Epoch 1/20.. Train loss: 62.756.. Validation loss: 4.698.. \n",
      "Epoch 1/20.. Train loss: 3.454.. Validation loss: 4.675.. \n",
      "Epoch 1/20.. Train loss: 1.278.. Validation loss: 4.674.. \n",
      "Epoch 1/20.. Train loss: 3.504.. Validation loss: 4.654.. \n",
      "Epoch 2/20.. Train loss: 7.233.. Validation loss: 4.591.. \n",
      "Epoch 2/20.. Train loss: 12.193.. Validation loss: 4.504.. \n",
      "Epoch 2/20.. Train loss: 0.717.. Validation loss: 4.501.. \n",
      "Epoch 2/20.. Train loss: 3.555.. Validation loss: 4.564.. \n",
      "Epoch 2/20.. Train loss: 3.437.. Validation loss: 4.645.. \n",
      "Epoch 2/20.. Train loss: 2.649.. Validation loss: 4.743.. \n",
      "Epoch 2/20.. Train loss: 0.499.. Validation loss: 4.79.. \n",
      "Epoch 2/20.. Train loss: 6.192.. Validation loss: 4.793.. \n",
      "Epoch 2/20.. Train loss: 0.124.. Validation loss: 4.77.. \n",
      "Epoch 2/20.. Train loss: 1.067.. Validation loss: 4.769.. \n",
      "Epoch 2/20.. Train loss: 0.778.. Validation loss: 4.797.. \n",
      "Epoch 2/20.. Train loss: 0.965.. Validation loss: 4.84.. \n",
      "Epoch 2/20.. Train loss: 2.086.. Validation loss: 4.84.. \n",
      "Epoch 2/20.. Train loss: 3.793.. Validation loss: 4.637.. \n",
      "Epoch 2/20.. Train loss: 6.878.. Validation loss: 4.621.. \n",
      "Epoch 2/20.. Train loss: 0.072.. Validation loss: 4.635.. \n",
      "Epoch 2/20.. Train loss: 8.578.. Validation loss: 4.672.. \n",
      "Epoch 2/20.. Train loss: 0.515.. Validation loss: 4.749.. \n",
      "Epoch 2/20.. Train loss: 1.710.. Validation loss: 4.584.. \n",
      "Epoch 2/20.. Train loss: 0.314.. Validation loss: 4.615.. \n",
      "Epoch 2/20.. Train loss: 0.741.. Validation loss: 4.69.. \n",
      "Epoch 2/20.. Train loss: 0.198.. Validation loss: 4.719.. \n",
      "Epoch 2/20.. Train loss: 0.288.. Validation loss: 4.702.. \n",
      "Epoch 2/20.. Train loss: 0.906.. Validation loss: 4.794.. \n",
      "Epoch 2/20.. Train loss: 5.885.. Validation loss: 4.934.. \n",
      "Epoch 2/20.. Train loss: 32.266.. Validation loss: 4.079.. \n",
      "Epoch 2/20.. Train loss: 2.816.. Validation loss: 4.14.. \n",
      "Epoch 2/20.. Train loss: 0.104.. Validation loss: 4.156.. \n",
      "Epoch 2/20.. Train loss: 2.047.. Validation loss: 4.221.. \n",
      "Epoch 2/20.. Train loss: 1.492.. Validation loss: 4.123.. \n",
      "Epoch 2/20.. Train loss: 0.961.. Validation loss: 4.085.. \n",
      "Epoch 2/20.. Train loss: 1.001.. Validation loss: 4.095.. \n",
      "Epoch 2/20.. Train loss: 6.693.. Validation loss: 4.12.. \n",
      "Epoch 2/20.. Train loss: 5.760.. Validation loss: 4.158.. \n",
      "Epoch 2/20.. Train loss: 4.348.. Validation loss: 4.202.. \n",
      "Epoch 2/20.. Train loss: 1.365.. Validation loss: 4.111.. \n",
      "Epoch 2/20.. Train loss: 5.430.. Validation loss: 4.16.. \n",
      "Epoch 2/20.. Train loss: 1.469.. Validation loss: 4.051.. \n",
      "Epoch 2/20.. Train loss: 6.991.. Validation loss: 4.081.. \n",
      "Epoch 2/20.. Train loss: 8.091.. Validation loss: 3.818.. \n",
      "Epoch 2/20.. Train loss: 0.094.. Validation loss: 3.827.. \n",
      "Epoch 2/20.. Train loss: 0.562.. Validation loss: 3.804.. \n",
      "Epoch 2/20.. Train loss: 0.728.. Validation loss: 3.775.. \n",
      "Epoch 2/20.. Train loss: 0.221.. Validation loss: 3.781.. \n",
      "Epoch 2/20.. Train loss: 0.831.. Validation loss: 3.793.. \n",
      "Epoch 2/20.. Train loss: 0.415.. Validation loss: 3.798.. \n",
      "Epoch 2/20.. Train loss: 1.538.. Validation loss: 3.815.. \n",
      "Epoch 2/20.. Train loss: 0.063.. Validation loss: 3.824.. \n",
      "Epoch 2/20.. Train loss: 0.028.. Validation loss: 3.83.. \n",
      "Epoch 2/20.. Train loss: 0.577.. Validation loss: 3.796.. \n",
      "Epoch 2/20.. Train loss: 8.636.. Validation loss: 3.822.. \n",
      "Epoch 2/20.. Train loss: 4.053.. Validation loss: 3.875.. \n",
      "Epoch 2/20.. Train loss: 1.358.. Validation loss: 3.803.. \n",
      "Epoch 2/20.. Train loss: 25.208.. Validation loss: 4.125.. \n",
      "Epoch 2/20.. Train loss: 2.922.. Validation loss: 3.927.. \n",
      "Epoch 2/20.. Train loss: 1.214.. Validation loss: 3.997.. \n",
      "Epoch 2/20.. Train loss: 0.387.. Validation loss: 3.954.. \n",
      "Epoch 2/20.. Train loss: 0.744.. Validation loss: 4.017.. \n",
      "Epoch 2/20.. Train loss: 1.428.. Validation loss: 4.13.. \n",
      "Epoch 2/20.. Train loss: 0.143.. Validation loss: 4.087.. \n",
      "Epoch 2/20.. Train loss: 0.339.. Validation loss: 4.121.. \n",
      "Epoch 2/20.. Train loss: 1.627.. Validation loss: 4.162.. \n",
      "Epoch 2/20.. Train loss: 0.284.. Validation loss: 4.119.. \n",
      "Epoch 2/20.. Train loss: 0.260.. Validation loss: 4.075.. \n",
      "Epoch 2/20.. Train loss: 17.456.. Validation loss: 3.632.. \n",
      "Epoch 2/20.. Train loss: 0.469.. Validation loss: 3.614.. \n",
      "Epoch 2/20.. Train loss: 2.741.. Validation loss: 3.65.. \n",
      "Epoch 2/20.. Train loss: 0.420.. Validation loss: 3.677.. \n",
      "Epoch 2/20.. Train loss: 0.066.. Validation loss: 3.691.. \n",
      "Epoch 2/20.. Train loss: 11.938.. Validation loss: 3.713.. \n",
      "Epoch 2/20.. Train loss: 2.170.. Validation loss: 3.827.. \n",
      "Epoch 2/20.. Train loss: 0.206.. Validation loss: 3.861.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20.. Train loss: 1.270.. Validation loss: 3.942.. \n",
      "Epoch 2/20.. Train loss: 0.020.. Validation loss: 3.926.. \n",
      "Epoch 2/20.. Train loss: 0.902.. Validation loss: 4.0.. \n",
      "Epoch 2/20.. Train loss: 0.398.. Validation loss: 4.069.. \n",
      "Epoch 2/20.. Train loss: 0.001.. Validation loss: 4.072.. \n",
      "Epoch 2/20.. Train loss: 4.999.. Validation loss: 4.136.. \n",
      "Epoch 2/20.. Train loss: 1.303.. Validation loss: 4.267.. \n",
      "Epoch 2/20.. Train loss: 0.202.. Validation loss: 4.194.. \n",
      "Epoch 2/20.. Train loss: 0.514.. Validation loss: 4.131.. \n",
      "Epoch 2/20.. Train loss: 1.189.. Validation loss: 4.221.. \n",
      "Epoch 2/20.. Train loss: 0.174.. Validation loss: 4.156.. \n",
      "Epoch 2/20.. Train loss: 0.012.. Validation loss: 4.149.. \n",
      "Epoch 2/20.. Train loss: 0.043.. Validation loss: 4.134.. \n",
      "Epoch 2/20.. Train loss: 37.491.. Validation loss: 5.3.. \n",
      "Epoch 2/20.. Train loss: 1.543.. Validation loss: 5.075.. \n",
      "Epoch 2/20.. Train loss: 0.069.. Validation loss: 5.117.. \n",
      "Epoch 2/20.. Train loss: 9.613.. Validation loss: 4.493.. \n",
      "Epoch 2/20.. Train loss: 0.364.. Validation loss: 4.474.. \n",
      "Epoch 2/20.. Train loss: 0.908.. Validation loss: 4.587.. \n",
      "Epoch 2/20.. Train loss: 43.208.. Validation loss: 3.245.. \n",
      "Epoch 2/20.. Train loss: 2.685.. Validation loss: 3.271.. \n",
      "Epoch 2/20.. Train loss: 0.732.. Validation loss: 3.29.. \n",
      "Epoch 2/20.. Train loss: 2.325.. Validation loss: 3.225.. \n",
      "Epoch 3/20.. Train loss: 4.009.. Validation loss: 3.214.. \n",
      "Epoch 3/20.. Train loss: 5.893.. Validation loss: 3.203.. \n",
      "Epoch 3/20.. Train loss: 0.099.. Validation loss: 3.207.. \n",
      "Epoch 3/20.. Train loss: 3.365.. Validation loss: 3.297.. \n",
      "Epoch 3/20.. Train loss: 2.512.. Validation loss: 3.39.. \n",
      "Epoch 3/20.. Train loss: 1.881.. Validation loss: 3.491.. \n",
      "Epoch 3/20.. Train loss: 0.296.. Validation loss: 3.532.. \n",
      "Epoch 3/20.. Train loss: 2.542.. Validation loss: 3.548.. \n",
      "Epoch 3/20.. Train loss: 0.421.. Validation loss: 3.501.. \n",
      "Epoch 3/20.. Train loss: 0.137.. Validation loss: 3.503.. \n",
      "Epoch 3/20.. Train loss: 0.269.. Validation loss: 3.521.. \n",
      "Epoch 3/20.. Train loss: 0.532.. Validation loss: 3.554.. \n",
      "Epoch 3/20.. Train loss: 0.976.. Validation loss: 3.557.. \n",
      "Epoch 3/20.. Train loss: 3.042.. Validation loss: 3.379.. \n",
      "Epoch 3/20.. Train loss: 2.890.. Validation loss: 3.381.. \n",
      "Epoch 3/20.. Train loss: 0.005.. Validation loss: 3.377.. \n",
      "Epoch 3/20.. Train loss: 4.304.. Validation loss: 3.411.. \n",
      "Epoch 3/20.. Train loss: 0.595.. Validation loss: 3.488.. \n",
      "Epoch 3/20.. Train loss: 0.919.. Validation loss: 3.375.. \n",
      "Epoch 3/20.. Train loss: 0.080.. Validation loss: 3.39.. \n",
      "Epoch 3/20.. Train loss: 0.643.. Validation loss: 3.457.. \n",
      "Epoch 3/20.. Train loss: 0.018.. Validation loss: 3.465.. \n",
      "Epoch 3/20.. Train loss: 0.812.. Validation loss: 3.438.. \n",
      "Epoch 3/20.. Train loss: 0.985.. Validation loss: 3.525.. \n",
      "Epoch 3/20.. Train loss: 5.281.. Validation loss: 3.649.. \n",
      "Epoch 3/20.. Train loss: 23.660.. Validation loss: 2.969.. \n",
      "Epoch 3/20.. Train loss: 2.630.. Validation loss: 3.032.. \n",
      "Epoch 3/20.. Train loss: 0.090.. Validation loss: 3.047.. \n",
      "Epoch 3/20.. Train loss: 1.853.. Validation loss: 3.113.. \n",
      "Epoch 3/20.. Train loss: 0.834.. Validation loss: 3.038.. \n",
      "Epoch 3/20.. Train loss: 0.911.. Validation loss: 2.997.. \n",
      "Epoch 3/20.. Train loss: 0.377.. Validation loss: 3.006.. \n",
      "Epoch 3/20.. Train loss: 4.291.. Validation loss: 3.036.. \n",
      "Epoch 3/20.. Train loss: 3.556.. Validation loss: 3.073.. \n",
      "Epoch 3/20.. Train loss: 2.533.. Validation loss: 3.111.. \n",
      "Epoch 3/20.. Train loss: 1.078.. Validation loss: 3.034.. \n",
      "Epoch 3/20.. Train loss: 3.373.. Validation loss: 3.076.. \n",
      "Epoch 3/20.. Train loss: 0.952.. Validation loss: 2.994.. \n",
      "Epoch 3/20.. Train loss: 5.417.. Validation loss: 3.025.. \n",
      "Epoch 3/20.. Train loss: 6.440.. Validation loss: 2.797.. \n",
      "Epoch 3/20.. Train loss: 0.089.. Validation loss: 2.806.. \n",
      "Epoch 3/20.. Train loss: 0.427.. Validation loss: 2.785.. \n",
      "Epoch 3/20.. Train loss: 0.227.. Validation loss: 2.767.. \n",
      "Epoch 3/20.. Train loss: 0.956.. Validation loss: 2.772.. \n",
      "Epoch 3/20.. Train loss: 0.639.. Validation loss: 2.787.. \n",
      "Epoch 3/20.. Train loss: 0.274.. Validation loss: 2.794.. \n",
      "Epoch 3/20.. Train loss: 0.871.. Validation loss: 2.81.. \n",
      "Epoch 3/20.. Train loss: 0.046.. Validation loss: 2.818.. \n",
      "Epoch 3/20.. Train loss: 0.016.. Validation loss: 2.823.. \n",
      "Epoch 3/20.. Train loss: 0.323.. Validation loss: 2.797.. \n",
      "Epoch 3/20.. Train loss: 5.160.. Validation loss: 2.828.. \n",
      "Epoch 3/20.. Train loss: 2.368.. Validation loss: 2.87.. \n",
      "Epoch 3/20.. Train loss: 1.039.. Validation loss: 2.813.. \n",
      "Epoch 3/20.. Train loss: 24.863.. Validation loss: 3.116.. \n",
      "Epoch 3/20.. Train loss: 2.184.. Validation loss: 2.953.. \n",
      "Epoch 3/20.. Train loss: 0.958.. Validation loss: 3.015.. \n",
      "Epoch 3/20.. Train loss: 0.469.. Validation loss: 2.968.. \n",
      "Epoch 3/20.. Train loss: 0.628.. Validation loss: 3.024.. \n",
      "Epoch 3/20.. Train loss: 1.553.. Validation loss: 3.138.. \n",
      "Epoch 3/20.. Train loss: 0.033.. Validation loss: 3.118.. \n",
      "Epoch 3/20.. Train loss: 0.086.. Validation loss: 3.135.. \n",
      "Epoch 3/20.. Train loss: 0.793.. Validation loss: 3.165.. \n",
      "Epoch 3/20.. Train loss: 0.390.. Validation loss: 3.116.. \n",
      "Epoch 3/20.. Train loss: 0.265.. Validation loss: 3.074.. \n",
      "Epoch 3/20.. Train loss: 14.341.. Validation loss: 2.696.. \n",
      "Epoch 3/20.. Train loss: 0.473.. Validation loss: 2.678.. \n",
      "Epoch 3/20.. Train loss: 2.424.. Validation loss: 2.712.. \n",
      "Epoch 3/20.. Train loss: 0.437.. Validation loss: 2.74.. \n",
      "Epoch 3/20.. Train loss: 0.154.. Validation loss: 2.76.. \n",
      "Epoch 3/20.. Train loss: 7.051.. Validation loss: 2.792.. \n",
      "Epoch 3/20.. Train loss: 2.376.. Validation loss: 2.906.. \n",
      "Epoch 3/20.. Train loss: 0.156.. Validation loss: 2.934.. \n",
      "Epoch 3/20.. Train loss: 1.194.. Validation loss: 3.011.. \n",
      "Epoch 3/20.. Train loss: 0.002.. Validation loss: 3.017.. \n",
      "Epoch 3/20.. Train loss: 0.797.. Validation loss: 3.086.. \n",
      "Epoch 3/20.. Train loss: 0.391.. Validation loss: 3.154.. \n",
      "Epoch 3/20.. Train loss: 0.003.. Validation loss: 3.16.. \n",
      "Epoch 3/20.. Train loss: 2.187.. Validation loss: 3.208.. \n",
      "Epoch 3/20.. Train loss: 1.432.. Validation loss: 3.34.. \n",
      "Epoch 3/20.. Train loss: 0.048.. Validation loss: 3.305.. \n",
      "Epoch 3/20.. Train loss: 0.845.. Validation loss: 3.224.. \n",
      "Epoch 3/20.. Train loss: 0.935.. Validation loss: 3.303.. \n",
      "Epoch 3/20.. Train loss: 0.036.. Validation loss: 3.274.. \n",
      "Epoch 3/20.. Train loss: 0.184.. Validation loss: 3.247.. \n",
      "Epoch 3/20.. Train loss: 0.181.. Validation loss: 3.215.. \n",
      "Epoch 3/20.. Train loss: 40.863.. Validation loss: 4.4.. \n",
      "Epoch 3/20.. Train loss: 1.696.. Validation loss: 4.166.. \n",
      "Epoch 3/20.. Train loss: 0.067.. Validation loss: 4.206.. \n",
      "Epoch 3/20.. Train loss: 7.388.. Validation loss: 3.666.. \n",
      "Epoch 3/20.. Train loss: 0.814.. Validation loss: 3.635.. \n",
      "Epoch 3/20.. Train loss: 0.822.. Validation loss: 3.744.. \n",
      "Epoch 3/20.. Train loss: 31.530.. Validation loss: 2.495.. \n",
      "Epoch 3/20.. Train loss: 2.160.. Validation loss: 2.549.. \n",
      "Epoch 3/20.. Train loss: 0.423.. Validation loss: 2.576.. \n",
      "Epoch 3/20.. Train loss: 1.640.. Validation loss: 2.49.. \n",
      "Epoch 4/20.. Train loss: 2.236.. Validation loss: 2.501.. \n",
      "Epoch 4/20.. Train loss: 2.686.. Validation loss: 2.519.. \n",
      "Epoch 4/20.. Train loss: 0.005.. Validation loss: 2.517.. \n",
      "Epoch 4/20.. Train loss: 3.203.. Validation loss: 2.626.. \n",
      "Epoch 4/20.. Train loss: 1.912.. Validation loss: 2.724.. \n",
      "Epoch 4/20.. Train loss: 1.392.. Validation loss: 2.822.. \n",
      "Epoch 4/20.. Train loss: 0.179.. Validation loss: 2.857.. \n",
      "Epoch 4/20.. Train loss: 0.887.. Validation loss: 2.873.. \n",
      "Epoch 4/20.. Train loss: 0.747.. Validation loss: 2.806.. \n",
      "Epoch 4/20.. Train loss: 0.012.. Validation loss: 2.805.. \n",
      "Epoch 4/20.. Train loss: 0.069.. Validation loss: 2.815.. \n",
      "Epoch 4/20.. Train loss: 0.303.. Validation loss: 2.841.. \n",
      "Epoch 4/20.. Train loss: 0.428.. Validation loss: 2.844.. \n",
      "Epoch 4/20.. Train loss: 2.553.. Validation loss: 2.683.. \n",
      "Epoch 4/20.. Train loss: 1.050.. Validation loss: 2.689.. \n",
      "Epoch 4/20.. Train loss: 0.102.. Validation loss: 2.672.. \n",
      "Epoch 4/20.. Train loss: 2.091.. Validation loss: 2.7.. \n",
      "Epoch 4/20.. Train loss: 0.647.. Validation loss: 2.775.. \n",
      "Epoch 4/20.. Train loss: 0.501.. Validation loss: 2.696.. \n",
      "Epoch 4/20.. Train loss: 0.007.. Validation loss: 2.701.. \n",
      "Epoch 4/20.. Train loss: 0.573.. Validation loss: 2.761.. \n",
      "Epoch 4/20.. Train loss: 0.008.. Validation loss: 2.756.. \n",
      "Epoch 4/20.. Train loss: 1.326.. Validation loss: 2.721.. \n",
      "Epoch 4/20.. Train loss: 1.034.. Validation loss: 2.804.. \n",
      "Epoch 4/20.. Train loss: 4.857.. Validation loss: 2.917.. \n",
      "Epoch 4/20.. Train loss: 18.248.. Validation loss: 2.352.. \n",
      "Epoch 4/20.. Train loss: 2.490.. Validation loss: 2.417.. \n",
      "Epoch 4/20.. Train loss: 0.079.. Validation loss: 2.432.. \n",
      "Epoch 4/20.. Train loss: 1.721.. Validation loss: 2.497.. \n",
      "Epoch 4/20.. Train loss: 0.479.. Validation loss: 2.439.. \n",
      "Epoch 4/20.. Train loss: 0.865.. Validation loss: 2.396.. \n",
      "Epoch 4/20.. Train loss: 0.114.. Validation loss: 2.402.. \n",
      "Epoch 4/20.. Train loss: 2.859.. Validation loss: 2.434.. \n",
      "Epoch 4/20.. Train loss: 2.271.. Validation loss: 2.468.. \n",
      "Epoch 4/20.. Train loss: 1.507.. Validation loss: 2.499.. \n",
      "Epoch 4/20.. Train loss: 0.892.. Validation loss: 2.432.. \n",
      "Epoch 4/20.. Train loss: 2.170.. Validation loss: 2.468.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20.. Train loss: 0.649.. Validation loss: 2.403.. \n",
      "Epoch 4/20.. Train loss: 4.416.. Validation loss: 2.433.. \n",
      "Epoch 4/20.. Train loss: 5.378.. Validation loss: 2.229.. \n",
      "Epoch 4/20.. Train loss: 0.085.. Validation loss: 2.239.. \n",
      "Epoch 4/20.. Train loss: 0.340.. Validation loss: 2.219.. \n",
      "Epoch 4/20.. Train loss: 0.041.. Validation loss: 2.211.. \n",
      "Epoch 4/20.. Train loss: 1.766.. Validation loss: 2.212.. \n",
      "Epoch 4/20.. Train loss: 0.509.. Validation loss: 2.228.. \n",
      "Epoch 4/20.. Train loss: 0.195.. Validation loss: 2.234.. \n",
      "Epoch 4/20.. Train loss: 0.501.. Validation loss: 2.249.. \n",
      "Epoch 4/20.. Train loss: 0.033.. Validation loss: 2.255.. \n",
      "Epoch 4/20.. Train loss: 0.009.. Validation loss: 2.259.. \n",
      "Epoch 4/20.. Train loss: 0.185.. Validation loss: 2.24.. \n",
      "Epoch 4/20.. Train loss: 3.150.. Validation loss: 2.27.. \n",
      "Epoch 4/20.. Train loss: 1.418.. Validation loss: 2.304.. \n",
      "Epoch 4/20.. Train loss: 0.832.. Validation loss: 2.256.. \n",
      "Epoch 4/20.. Train loss: 24.625.. Validation loss: 2.546.. \n",
      "Epoch 4/20.. Train loss: 1.717.. Validation loss: 2.406.. \n",
      "Epoch 4/20.. Train loss: 0.795.. Validation loss: 2.462.. \n",
      "Epoch 4/20.. Train loss: 0.531.. Validation loss: 2.414.. \n",
      "Epoch 4/20.. Train loss: 0.552.. Validation loss: 2.465.. \n",
      "Epoch 4/20.. Train loss: 1.642.. Validation loss: 2.578.. \n",
      "Epoch 4/20.. Train loss: 0.001.. Validation loss: 2.574.. \n",
      "Epoch 4/20.. Train loss: 0.007.. Validation loss: 2.579.. \n",
      "Epoch 4/20.. Train loss: 0.375.. Validation loss: 2.601.. \n",
      "Epoch 4/20.. Train loss: 0.475.. Validation loss: 2.548.. \n",
      "Epoch 4/20.. Train loss: 0.269.. Validation loss: 2.507.. \n",
      "Epoch 4/20.. Train loss: 12.253.. Validation loss: 2.174.. \n",
      "Epoch 4/20.. Train loss: 0.468.. Validation loss: 2.156.. \n",
      "Epoch 4/20.. Train loss: 2.197.. Validation loss: 2.189.. \n",
      "Epoch 4/20.. Train loss: 0.447.. Validation loss: 2.217.. \n",
      "Epoch 4/20.. Train loss: 0.239.. Validation loss: 2.242.. \n",
      "Epoch 4/20.. Train loss: 4.254.. Validation loss: 2.274.. \n",
      "Epoch 4/20.. Train loss: 2.530.. Validation loss: 2.388.. \n",
      "Epoch 4/20.. Train loss: 0.126.. Validation loss: 2.413.. \n",
      "Epoch 4/20.. Train loss: 1.147.. Validation loss: 2.487.. \n",
      "Epoch 4/20.. Train loss: 0.034.. Validation loss: 2.508.. \n",
      "Epoch 4/20.. Train loss: 0.732.. Validation loss: 2.574.. \n",
      "Epoch 4/20.. Train loss: 0.383.. Validation loss: 2.64.. \n",
      "Epoch 4/20.. Train loss: 0.005.. Validation loss: 2.647.. \n",
      "Epoch 4/20.. Train loss: 0.847.. Validation loss: 2.68.. \n",
      "Epoch 4/20.. Train loss: 1.526.. Validation loss: 2.812.. \n",
      "Epoch 4/20.. Train loss: 0.003.. Validation loss: 2.804.. \n",
      "Epoch 4/20.. Train loss: 1.140.. Validation loss: 2.71.. \n",
      "Epoch 4/20.. Train loss: 0.765.. Validation loss: 2.781.. \n",
      "Epoch 4/20.. Train loss: 0.001.. Validation loss: 2.777.. \n",
      "Epoch 4/20.. Train loss: 0.431.. Validation loss: 2.734.. \n",
      "Epoch 4/20.. Train loss: 0.335.. Validation loss: 2.692.. \n",
      "Epoch 4/20.. Train loss: 43.420.. Validation loss: 3.888.. \n",
      "Epoch 4/20.. Train loss: 1.819.. Validation loss: 3.646.. \n",
      "Epoch 4/20.. Train loss: 0.065.. Validation loss: 3.686.. \n",
      "Epoch 4/20.. Train loss: 5.948.. Validation loss: 3.205.. \n",
      "Epoch 4/20.. Train loss: 1.224.. Validation loss: 3.165.. \n",
      "Epoch 4/20.. Train loss: 0.765.. Validation loss: 3.271.. \n",
      "Epoch 4/20.. Train loss: 24.163.. Validation loss: 2.112.. \n",
      "Epoch 4/20.. Train loss: 1.810.. Validation loss: 2.183.. \n",
      "Epoch 4/20.. Train loss: 0.251.. Validation loss: 2.21.. \n",
      "Epoch 4/20.. Train loss: 1.218.. Validation loss: 2.116.. \n",
      "Epoch 5/20.. Train loss: 1.260.. Validation loss: 2.136.. \n",
      "Epoch 5/20.. Train loss: 1.114.. Validation loss: 2.159.. \n",
      "Epoch 5/20.. Train loss: 0.116.. Validation loss: 2.149.. \n",
      "Epoch 5/20.. Train loss: 3.077.. Validation loss: 2.271.. \n",
      "Epoch 5/20.. Train loss: 1.523.. Validation loss: 2.368.. \n",
      "Epoch 5/20.. Train loss: 1.080.. Validation loss: 2.461.. \n",
      "Epoch 5/20.. Train loss: 0.111.. Validation loss: 2.49.. \n",
      "Epoch 5/20.. Train loss: 0.218.. Validation loss: 2.501.. \n",
      "Epoch 5/20.. Train loss: 1.037.. Validation loss: 2.418.. \n",
      "Epoch 5/20.. Train loss: 0.203.. Validation loss: 2.413.. \n",
      "Epoch 5/20.. Train loss: 0.007.. Validation loss: 2.416.. \n",
      "Epoch 5/20.. Train loss: 0.182.. Validation loss: 2.437.. \n",
      "Epoch 5/20.. Train loss: 0.168.. Validation loss: 2.44.. \n",
      "Epoch 5/20.. Train loss: 2.220.. Validation loss: 2.29.. \n",
      "Epoch 5/20.. Train loss: 0.281.. Validation loss: 2.295.. \n",
      "Epoch 5/20.. Train loss: 0.251.. Validation loss: 2.269.. \n",
      "Epoch 5/20.. Train loss: 0.966.. Validation loss: 2.29.. \n",
      "Epoch 5/20.. Train loss: 0.683.. Validation loss: 2.364.. \n",
      "Epoch 5/20.. Train loss: 0.276.. Validation loss: 2.307.. \n",
      "Epoch 5/20.. Train loss: 0.004.. Validation loss: 2.304.. \n",
      "Epoch 5/20.. Train loss: 0.526.. Validation loss: 2.361.. \n",
      "Epoch 5/20.. Train loss: 0.064.. Validation loss: 2.346.. \n",
      "Epoch 5/20.. Train loss: 1.741.. Validation loss: 2.306.. \n",
      "Epoch 5/20.. Train loss: 1.062.. Validation loss: 2.386.. \n",
      "Epoch 5/20.. Train loss: 4.559.. Validation loss: 2.491.. \n",
      "Epoch 5/20.. Train loss: 14.742.. Validation loss: 2.004.. \n",
      "Epoch 5/20.. Train loss: 2.387.. Validation loss: 2.07.. \n",
      "Epoch 5/20.. Train loss: 0.071.. Validation loss: 2.085.. \n",
      "Epoch 5/20.. Train loss: 1.632.. Validation loss: 2.15.. \n",
      "Epoch 5/20.. Train loss: 0.284.. Validation loss: 2.104.. \n",
      "Epoch 5/20.. Train loss: 0.821.. Validation loss: 2.06.. \n",
      "Epoch 5/20.. Train loss: 0.020.. Validation loss: 2.063.. \n",
      "Epoch 5/20.. Train loss: 1.986.. Validation loss: 2.093.. \n",
      "Epoch 5/20.. Train loss: 1.508.. Validation loss: 2.123.. \n",
      "Epoch 5/20.. Train loss: 0.919.. Validation loss: 2.149.. \n",
      "Epoch 5/20.. Train loss: 0.767.. Validation loss: 2.088.. \n",
      "Epoch 5/20.. Train loss: 1.453.. Validation loss: 2.119.. \n",
      "Epoch 5/20.. Train loss: 0.466.. Validation loss: 2.066.. \n",
      "Epoch 5/20.. Train loss: 3.769.. Validation loss: 2.096.. \n",
      "Epoch 5/20.. Train loss: 4.680.. Validation loss: 1.909.. \n",
      "Epoch 5/20.. Train loss: 0.081.. Validation loss: 1.919.. \n",
      "Epoch 5/20.. Train loss: 0.284.. Validation loss: 1.901.. \n",
      "Epoch 5/20.. Train loss: 0.000.. Validation loss: 1.9.. \n",
      "Epoch 5/20.. Train loss: 2.456.. Validation loss: 1.895.. \n",
      "Epoch 5/20.. Train loss: 0.420.. Validation loss: 1.912.. \n",
      "Epoch 5/20.. Train loss: 0.152.. Validation loss: 1.918.. \n",
      "Epoch 5/20.. Train loss: 0.293.. Validation loss: 1.93.. \n",
      "Epoch 5/20.. Train loss: 0.024.. Validation loss: 1.936.. \n",
      "Epoch 5/20.. Train loss: 0.005.. Validation loss: 1.939.. \n",
      "Epoch 5/20.. Train loss: 0.109.. Validation loss: 1.924.. \n",
      "Epoch 5/20.. Train loss: 1.969.. Validation loss: 1.952.. \n",
      "Epoch 5/20.. Train loss: 0.873.. Validation loss: 1.978.. \n",
      "Epoch 5/20.. Train loss: 0.694.. Validation loss: 1.937.. \n",
      "Epoch 5/20.. Train loss: 24.465.. Validation loss: 2.217.. \n",
      "Epoch 5/20.. Train loss: 1.414.. Validation loss: 2.094.. \n",
      "Epoch 5/20.. Train loss: 0.689.. Validation loss: 2.145.. \n",
      "Epoch 5/20.. Train loss: 0.575.. Validation loss: 2.095.. \n",
      "Epoch 5/20.. Train loss: 0.502.. Validation loss: 2.143.. \n",
      "Epoch 5/20.. Train loss: 1.702.. Validation loss: 2.256.. \n",
      "Epoch 5/20.. Train loss: 0.005.. Validation loss: 2.263.. \n",
      "Epoch 5/20.. Train loss: 0.004.. Validation loss: 2.259.. \n",
      "Epoch 5/20.. Train loss: 0.170.. Validation loss: 2.274.. \n",
      "Epoch 5/20.. Train loss: 0.539.. Validation loss: 2.22.. \n",
      "Epoch 5/20.. Train loss: 0.271.. Validation loss: 2.18.. \n",
      "Epoch 5/20.. Train loss: 10.822.. Validation loss: 1.878.. \n",
      "Epoch 5/20.. Train loss: 0.457.. Validation loss: 1.86.. \n",
      "Epoch 5/20.. Train loss: 2.032.. Validation loss: 1.893.. \n",
      "Epoch 5/20.. Train loss: 0.452.. Validation loss: 1.921.. \n",
      "Epoch 5/20.. Train loss: 0.310.. Validation loss: 1.948.. \n",
      "Epoch 5/20.. Train loss: 2.627.. Validation loss: 1.978.. \n",
      "Epoch 5/20.. Train loss: 2.642.. Validation loss: 2.091.. \n",
      "Epoch 5/20.. Train loss: 0.108.. Validation loss: 2.114.. \n",
      "Epoch 5/20.. Train loss: 1.120.. Validation loss: 2.186.. \n",
      "Epoch 5/20.. Train loss: 0.081.. Validation loss: 2.217.. \n",
      "Epoch 5/20.. Train loss: 0.693.. Validation loss: 2.281.. \n",
      "Epoch 5/20.. Train loss: 0.375.. Validation loss: 2.346.. \n",
      "Epoch 5/20.. Train loss: 0.007.. Validation loss: 2.354.. \n",
      "Epoch 5/20.. Train loss: 0.257.. Validation loss: 2.373.. \n",
      "Epoch 5/20.. Train loss: 1.591.. Validation loss: 2.505.. \n",
      "Epoch 5/20.. Train loss: 0.005.. Validation loss: 2.516.. \n",
      "Epoch 5/20.. Train loss: 1.386.. Validation loss: 2.413.. \n",
      "Epoch 5/20.. Train loss: 0.649.. Validation loss: 2.477.. \n",
      "Epoch 5/20.. Train loss: 0.009.. Validation loss: 2.49.. \n",
      "Epoch 5/20.. Train loss: 0.664.. Validation loss: 2.437.. \n",
      "Epoch 5/20.. Train loss: 0.468.. Validation loss: 2.387.. \n",
      "Epoch 5/20.. Train loss: 45.342.. Validation loss: 3.589.. \n",
      "Epoch 5/20.. Train loss: 1.919.. Validation loss: 3.342.. \n",
      "Epoch 5/20.. Train loss: 0.062.. Validation loss: 3.381.. \n",
      "Epoch 5/20.. Train loss: 4.994.. Validation loss: 2.944.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20.. Train loss: 1.541.. Validation loss: 2.897.. \n",
      "Epoch 5/20.. Train loss: 0.726.. Validation loss: 3.001.. \n",
      "Epoch 5/20.. Train loss: 19.398.. Validation loss: 1.92.. \n",
      "Epoch 5/20.. Train loss: 1.571.. Validation loss: 2.001.. \n",
      "Epoch 5/20.. Train loss: 0.155.. Validation loss: 2.026.. \n",
      "Epoch 5/20.. Train loss: 0.952.. Validation loss: 1.93.. \n",
      "Epoch 6/20.. Train loss: 0.719.. Validation loss: 1.95.. \n",
      "Epoch 6/20.. Train loss: 0.386.. Validation loss: 1.969.. \n",
      "Epoch 6/20.. Train loss: 0.284.. Validation loss: 1.95.. \n",
      "Epoch 6/20.. Train loss: 2.979.. Validation loss: 2.081.. \n",
      "Epoch 6/20.. Train loss: 1.264.. Validation loss: 2.176.. \n",
      "Epoch 6/20.. Train loss: 0.876.. Validation loss: 2.265.. \n",
      "Epoch 6/20.. Train loss: 0.072.. Validation loss: 2.289.. \n",
      "Epoch 6/20.. Train loss: 0.014.. Validation loss: 2.292.. \n",
      "Epoch 6/20.. Train loss: 1.271.. Validation loss: 2.198.. \n",
      "Epoch 6/20.. Train loss: 0.483.. Validation loss: 2.188.. \n",
      "Epoch 6/20.. Train loss: 0.001.. Validation loss: 2.187.. \n",
      "Epoch 6/20.. Train loss: 0.117.. Validation loss: 2.203.. \n",
      "Epoch 6/20.. Train loss: 0.054.. Validation loss: 2.205.. \n",
      "Epoch 6/20.. Train loss: 1.988.. Validation loss: 2.065.. \n",
      "Epoch 6/20.. Train loss: 0.028.. Validation loss: 2.067.. \n",
      "Epoch 6/20.. Train loss: 0.400.. Validation loss: 2.033.. \n",
      "Epoch 6/20.. Train loss: 0.411.. Validation loss: 2.048.. \n",
      "Epoch 6/20.. Train loss: 0.705.. Validation loss: 2.121.. \n",
      "Epoch 6/20.. Train loss: 0.155.. Validation loss: 2.08.. \n",
      "Epoch 6/20.. Train loss: 0.028.. Validation loss: 2.071.. \n",
      "Epoch 6/20.. Train loss: 0.493.. Validation loss: 2.124.. \n",
      "Epoch 6/20.. Train loss: 0.138.. Validation loss: 2.103.. \n",
      "Epoch 6/20.. Train loss: 2.039.. Validation loss: 2.059.. \n",
      "Epoch 6/20.. Train loss: 1.075.. Validation loss: 2.137.. \n",
      "Epoch 6/20.. Train loss: 4.346.. Validation loss: 2.237.. \n",
      "Epoch 6/20.. Train loss: 12.419.. Validation loss: 1.803.. \n",
      "Epoch 6/20.. Train loss: 2.309.. Validation loss: 1.87.. \n",
      "Epoch 6/20.. Train loss: 0.065.. Validation loss: 1.885.. \n",
      "Epoch 6/20.. Train loss: 1.574.. Validation loss: 1.95.. \n",
      "Epoch 6/20.. Train loss: 0.175.. Validation loss: 1.914.. \n",
      "Epoch 6/20.. Train loss: 0.779.. Validation loss: 1.869.. \n",
      "Epoch 6/20.. Train loss: 0.000.. Validation loss: 1.869.. \n",
      "Epoch 6/20.. Train loss: 1.439.. Validation loss: 1.897.. \n",
      "Epoch 6/20.. Train loss: 1.043.. Validation loss: 1.924.. \n",
      "Epoch 6/20.. Train loss: 0.576.. Validation loss: 1.945.. \n",
      "Epoch 6/20.. Train loss: 0.682.. Validation loss: 1.889.. \n",
      "Epoch 6/20.. Train loss: 1.014.. Validation loss: 1.916.. \n",
      "Epoch 6/20.. Train loss: 0.352.. Validation loss: 1.87.. \n",
      "Epoch 6/20.. Train loss: 3.347.. Validation loss: 1.9.. \n",
      "Epoch 6/20.. Train loss: 4.215.. Validation loss: 1.725.. \n",
      "Epoch 6/20.. Train loss: 0.077.. Validation loss: 1.735.. \n",
      "Epoch 6/20.. Train loss: 0.247.. Validation loss: 1.717.. \n",
      "Epoch 6/20.. Train loss: 0.021.. Validation loss: 1.723.. \n",
      "Epoch 6/20.. Train loss: 2.971.. Validation loss: 1.713.. \n",
      "Epoch 6/20.. Train loss: 0.356.. Validation loss: 1.729.. \n",
      "Epoch 6/20.. Train loss: 0.128.. Validation loss: 1.736.. \n",
      "Epoch 6/20.. Train loss: 0.175.. Validation loss: 1.746.. \n",
      "Epoch 6/20.. Train loss: 0.018.. Validation loss: 1.751.. \n",
      "Epoch 6/20.. Train loss: 0.002.. Validation loss: 1.753.. \n",
      "Epoch 6/20.. Train loss: 0.066.. Validation loss: 1.741.. \n",
      "Epoch 6/20.. Train loss: 1.260.. Validation loss: 1.765.. \n",
      "Epoch 6/20.. Train loss: 0.556.. Validation loss: 1.786.. \n",
      "Epoch 6/20.. Train loss: 0.600.. Validation loss: 1.75.. \n",
      "Epoch 6/20.. Train loss: 24.360.. Validation loss: 2.022.. \n",
      "Epoch 6/20.. Train loss: 1.214.. Validation loss: 1.911.. \n",
      "Epoch 6/20.. Train loss: 0.620.. Validation loss: 1.959.. \n",
      "Epoch 6/20.. Train loss: 0.606.. Validation loss: 1.909.. \n",
      "Epoch 6/20.. Train loss: 0.469.. Validation loss: 1.954.. \n",
      "Epoch 6/20.. Train loss: 1.741.. Validation loss: 2.066.. \n",
      "Epoch 6/20.. Train loss: 0.023.. Validation loss: 2.081.. \n",
      "Epoch 6/20.. Train loss: 0.029.. Validation loss: 2.071.. \n",
      "Epoch 6/20.. Train loss: 0.072.. Validation loss: 2.081.. \n",
      "Epoch 6/20.. Train loss: 0.586.. Validation loss: 2.025.. \n",
      "Epoch 6/20.. Train loss: 0.273.. Validation loss: 1.986.. \n",
      "Epoch 6/20.. Train loss: 9.823.. Validation loss: 1.707.. \n",
      "Epoch 6/20.. Train loss: 0.441.. Validation loss: 1.689.. \n",
      "Epoch 6/20.. Train loss: 1.908.. Validation loss: 1.721.. \n",
      "Epoch 6/20.. Train loss: 0.453.. Validation loss: 1.749.. \n",
      "Epoch 6/20.. Train loss: 0.365.. Validation loss: 1.779.. \n",
      "Epoch 6/20.. Train loss: 1.664.. Validation loss: 1.805.. \n",
      "Epoch 6/20.. Train loss: 2.722.. Validation loss: 1.917.. \n",
      "Epoch 6/20.. Train loss: 0.096.. Validation loss: 1.939.. \n",
      "Epoch 6/20.. Train loss: 1.106.. Validation loss: 2.009.. \n",
      "Epoch 6/20.. Train loss: 0.128.. Validation loss: 2.047.. \n",
      "Epoch 6/20.. Train loss: 0.671.. Validation loss: 2.11.. \n",
      "Epoch 6/20.. Train loss: 0.368.. Validation loss: 2.174.. \n",
      "Epoch 6/20.. Train loss: 0.008.. Validation loss: 2.183.. \n",
      "Epoch 6/20.. Train loss: 0.040.. Validation loss: 2.191.. \n",
      "Epoch 6/20.. Train loss: 1.636.. Validation loss: 2.322.. \n",
      "Epoch 6/20.. Train loss: 0.024.. Validation loss: 2.345.. \n",
      "Epoch 6/20.. Train loss: 1.584.. Validation loss: 2.235.. \n",
      "Epoch 6/20.. Train loss: 0.568.. Validation loss: 2.294.. \n",
      "Epoch 6/20.. Train loss: 0.032.. Validation loss: 2.32.. \n",
      "Epoch 6/20.. Train loss: 0.853.. Validation loss: 2.26.. \n",
      "Epoch 6/20.. Train loss: 0.570.. Validation loss: 2.205.. \n",
      "Epoch 6/20.. Train loss: 46.778.. Validation loss: 3.411.. \n",
      "Epoch 6/20.. Train loss: 2.001.. Validation loss: 3.16.. \n",
      "Epoch 6/20.. Train loss: 0.060.. Validation loss: 3.197.. \n",
      "Epoch 6/20.. Train loss: 4.349.. Validation loss: 2.792.. \n",
      "Epoch 6/20.. Train loss: 1.760.. Validation loss: 2.74.. \n",
      "Epoch 6/20.. Train loss: 0.701.. Validation loss: 2.843.. \n",
      "Epoch 6/20.. Train loss: 16.243.. Validation loss: 1.826.. \n",
      "Epoch 6/20.. Train loss: 1.404.. Validation loss: 1.912.. \n",
      "Epoch 6/20.. Train loss: 0.099.. Validation loss: 1.934.. \n",
      "Epoch 6/20.. Train loss: 0.780.. Validation loss: 1.839.. \n",
      "Epoch 7/20.. Train loss: 0.415.. Validation loss: 1.858.. \n",
      "Epoch 7/20.. Train loss: 0.088.. Validation loss: 1.869.. \n",
      "Epoch 7/20.. Train loss: 0.445.. Validation loss: 1.842.. \n",
      "Epoch 7/20.. Train loss: 2.899.. Validation loss: 1.978.. \n",
      "Epoch 7/20.. Train loss: 1.088.. Validation loss: 2.071.. \n",
      "Epoch 7/20.. Train loss: 0.739.. Validation loss: 2.156.. \n",
      "Epoch 7/20.. Train loss: 0.049.. Validation loss: 2.176.. \n",
      "Epoch 7/20.. Train loss: 0.017.. Validation loss: 2.173.. \n",
      "Epoch 7/20.. Train loss: 1.451.. Validation loss: 2.07.. \n",
      "Epoch 7/20.. Train loss: 0.755.. Validation loss: 2.056.. \n",
      "Epoch 7/20.. Train loss: 0.013.. Validation loss: 2.051.. \n",
      "Epoch 7/20.. Train loss: 0.082.. Validation loss: 2.065.. \n",
      "Epoch 7/20.. Train loss: 0.010.. Validation loss: 2.066.. \n",
      "Epoch 7/20.. Train loss: 1.824.. Validation loss: 1.932.. \n",
      "Epoch 7/20.. Train loss: 0.010.. Validation loss: 1.931.. \n",
      "Epoch 7/20.. Train loss: 0.529.. Validation loss: 1.892.. \n",
      "Epoch 7/20.. Train loss: 0.150.. Validation loss: 1.902.. \n",
      "Epoch 7/20.. Train loss: 0.717.. Validation loss: 1.973.. \n",
      "Epoch 7/20.. Train loss: 0.089.. Validation loss: 1.943.. \n",
      "Epoch 7/20.. Train loss: 0.059.. Validation loss: 1.93.. \n",
      "Epoch 7/20.. Train loss: 0.470.. Validation loss: 1.982.. \n",
      "Epoch 7/20.. Train loss: 0.209.. Validation loss: 1.955.. \n",
      "Epoch 7/20.. Train loss: 2.231.. Validation loss: 1.91.. \n",
      "Epoch 7/20.. Train loss: 1.078.. Validation loss: 1.985.. \n",
      "Epoch 7/20.. Train loss: 4.193.. Validation loss: 2.081.. \n",
      "Epoch 7/20.. Train loss: 10.847.. Validation loss: 1.686.. \n",
      "Epoch 7/20.. Train loss: 2.249.. Validation loss: 1.753.. \n",
      "Epoch 7/20.. Train loss: 0.060.. Validation loss: 1.767.. \n",
      "Epoch 7/20.. Train loss: 1.537.. Validation loss: 1.832.. \n",
      "Epoch 7/20.. Train loss: 0.114.. Validation loss: 1.803.. \n",
      "Epoch 7/20.. Train loss: 0.740.. Validation loss: 1.758.. \n",
      "Epoch 7/20.. Train loss: 0.009.. Validation loss: 1.756.. \n",
      "Epoch 7/20.. Train loss: 1.087.. Validation loss: 1.782.. \n",
      "Epoch 7/20.. Train loss: 0.753.. Validation loss: 1.805.. \n",
      "Epoch 7/20.. Train loss: 0.372.. Validation loss: 1.823.. \n",
      "Epoch 7/20.. Train loss: 0.624.. Validation loss: 1.77.. \n",
      "Epoch 7/20.. Train loss: 0.739.. Validation loss: 1.793.. \n",
      "Epoch 7/20.. Train loss: 0.281.. Validation loss: 1.754.. \n",
      "Epoch 7/20.. Train loss: 3.069.. Validation loss: 1.783.. \n",
      "Epoch 7/20.. Train loss: 3.903.. Validation loss: 1.616.. \n",
      "Epoch 7/20.. Train loss: 0.074.. Validation loss: 1.626.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20.. Train loss: 0.222.. Validation loss: 1.609.. \n",
      "Epoch 7/20.. Train loss: 0.063.. Validation loss: 1.62.. \n",
      "Epoch 7/20.. Train loss: 3.319.. Validation loss: 1.606.. \n",
      "Epoch 7/20.. Train loss: 0.309.. Validation loss: 1.622.. \n",
      "Epoch 7/20.. Train loss: 0.116.. Validation loss: 1.628.. \n",
      "Epoch 7/20.. Train loss: 0.107.. Validation loss: 1.636.. \n",
      "Epoch 7/20.. Train loss: 0.013.. Validation loss: 1.641.. \n",
      "Epoch 7/20.. Train loss: 0.001.. Validation loss: 1.642.. \n",
      "Epoch 7/20.. Train loss: 0.042.. Validation loss: 1.633.. \n",
      "Epoch 7/20.. Train loss: 0.827.. Validation loss: 1.653.. \n",
      "Epoch 7/20.. Train loss: 0.366.. Validation loss: 1.671.. \n",
      "Epoch 7/20.. Train loss: 0.535.. Validation loss: 1.638.. \n",
      "Epoch 7/20.. Train loss: 24.295.. Validation loss: 1.903.. \n",
      "Epoch 7/20.. Train loss: 1.080.. Validation loss: 1.8.. \n",
      "Epoch 7/20.. Train loss: 0.575.. Validation loss: 1.846.. \n",
      "Epoch 7/20.. Train loss: 0.625.. Validation loss: 1.796.. \n",
      "Epoch 7/20.. Train loss: 0.448.. Validation loss: 1.84.. \n",
      "Epoch 7/20.. Train loss: 1.764.. Validation loss: 1.95.. \n",
      "Epoch 7/20.. Train loss: 0.045.. Validation loss: 1.972.. \n",
      "Epoch 7/20.. Train loss: 0.060.. Validation loss: 1.957.. \n",
      "Epoch 7/20.. Train loss: 0.028.. Validation loss: 1.963.. \n",
      "Epoch 7/20.. Train loss: 0.619.. Validation loss: 1.907.. \n",
      "Epoch 7/20.. Train loss: 0.273.. Validation loss: 1.869.. \n",
      "Epoch 7/20.. Train loss: 9.115.. Validation loss: 1.605.. \n",
      "Epoch 7/20.. Train loss: 0.424.. Validation loss: 1.588.. \n",
      "Epoch 7/20.. Train loss: 1.814.. Validation loss: 1.619.. \n",
      "Epoch 7/20.. Train loss: 0.452.. Validation loss: 1.647.. \n",
      "Epoch 7/20.. Train loss: 0.405.. Validation loss: 1.678.. \n",
      "Epoch 7/20.. Train loss: 1.081.. Validation loss: 1.701.. \n",
      "Epoch 7/20.. Train loss: 2.779.. Validation loss: 1.812.. \n",
      "Epoch 7/20.. Train loss: 0.090.. Validation loss: 1.833.. \n",
      "Epoch 7/20.. Train loss: 1.102.. Validation loss: 1.902.. \n",
      "Epoch 7/20.. Train loss: 0.167.. Validation loss: 1.946.. \n",
      "Epoch 7/20.. Train loss: 0.660.. Validation loss: 2.008.. \n",
      "Epoch 7/20.. Train loss: 0.360.. Validation loss: 2.071.. \n",
      "Epoch 7/20.. Train loss: 0.008.. Validation loss: 2.08.. \n",
      "Epoch 7/20.. Train loss: 0.001.. Validation loss: 2.078.. \n",
      "Epoch 7/20.. Train loss: 1.665.. Validation loss: 2.209.. \n",
      "Epoch 7/20.. Train loss: 0.048.. Validation loss: 2.241.. \n",
      "Epoch 7/20.. Train loss: 1.739.. Validation loss: 2.126.. \n",
      "Epoch 7/20.. Train loss: 0.509.. Validation loss: 2.182.. \n",
      "Epoch 7/20.. Train loss: 0.058.. Validation loss: 2.216.. \n",
      "Epoch 7/20.. Train loss: 0.993.. Validation loss: 2.151.. \n",
      "Epoch 7/20.. Train loss: 0.641.. Validation loss: 2.093.. \n",
      "Epoch 7/20.. Train loss: 47.848.. Validation loss: 3.301.. \n",
      "Epoch 7/20.. Train loss: 2.069.. Validation loss: 3.046.. \n",
      "Epoch 7/20.. Train loss: 0.058.. Validation loss: 3.083.. \n",
      "Epoch 7/20.. Train loss: 3.905.. Validation loss: 2.701.. \n",
      "Epoch 7/20.. Train loss: 1.895.. Validation loss: 2.646.. \n",
      "Epoch 7/20.. Train loss: 0.685.. Validation loss: 2.748.. \n",
      "Epoch 7/20.. Train loss: 14.109.. Validation loss: 1.781.. \n",
      "Epoch 7/20.. Train loss: 1.285.. Validation loss: 1.87.. \n",
      "Epoch 7/20.. Train loss: 0.067.. Validation loss: 1.89.. \n",
      "Epoch 7/20.. Train loss: 0.666.. Validation loss: 1.796.. \n",
      "Epoch 8/20.. Train loss: 0.243.. Validation loss: 1.813.. \n",
      "Epoch 8/20.. Train loss: 0.003.. Validation loss: 1.815.. \n",
      "Epoch 8/20.. Train loss: 0.576.. Validation loss: 1.782.. \n",
      "Epoch 8/20.. Train loss: 2.834.. Validation loss: 1.922.. \n",
      "Epoch 8/20.. Train loss: 0.965.. Validation loss: 2.012.. \n",
      "Epoch 8/20.. Train loss: 0.646.. Validation loss: 2.093.. \n",
      "Epoch 8/20.. Train loss: 0.034.. Validation loss: 2.111.. \n",
      "Epoch 8/20.. Train loss: 0.100.. Validation loss: 2.102.. \n",
      "Epoch 8/20.. Train loss: 1.582.. Validation loss: 1.993.. \n",
      "Epoch 8/20.. Train loss: 0.981.. Validation loss: 1.976.. \n",
      "Epoch 8/20.. Train loss: 0.026.. Validation loss: 1.969.. \n",
      "Epoch 8/20.. Train loss: 0.064.. Validation loss: 1.982.. \n",
      "Epoch 8/20.. Train loss: 0.000.. Validation loss: 1.982.. \n",
      "Epoch 8/20.. Train loss: 1.706.. Validation loss: 1.852.. \n",
      "Epoch 8/20.. Train loss: 0.087.. Validation loss: 1.848.. \n",
      "Epoch 8/20.. Train loss: 0.634.. Validation loss: 1.805.. \n",
      "Epoch 8/20.. Train loss: 0.039.. Validation loss: 1.81.. \n",
      "Epoch 8/20.. Train loss: 0.723.. Validation loss: 1.881.. \n",
      "Epoch 8/20.. Train loss: 0.052.. Validation loss: 1.858.. \n",
      "Epoch 8/20.. Train loss: 0.087.. Validation loss: 1.842.. \n",
      "Epoch 8/20.. Train loss: 0.454.. Validation loss: 1.892.. \n",
      "Epoch 8/20.. Train loss: 0.269.. Validation loss: 1.862.. \n",
      "Epoch 8/20.. Train loss: 2.338.. Validation loss: 1.817.. \n",
      "Epoch 8/20.. Train loss: 1.072.. Validation loss: 1.89.. \n",
      "Epoch 8/20.. Train loss: 4.082.. Validation loss: 1.983.. \n",
      "Epoch 8/20.. Train loss: 9.764.. Validation loss: 1.614.. \n",
      "Epoch 8/20.. Train loss: 2.203.. Validation loss: 1.682.. \n",
      "Epoch 8/20.. Train loss: 0.056.. Validation loss: 1.696.. \n",
      "Epoch 8/20.. Train loss: 1.515.. Validation loss: 1.761.. \n",
      "Epoch 8/20.. Train loss: 0.078.. Validation loss: 1.737.. \n",
      "Epoch 8/20.. Train loss: 0.703.. Validation loss: 1.692.. \n",
      "Epoch 8/20.. Train loss: 0.026.. Validation loss: 1.688.. \n",
      "Epoch 8/20.. Train loss: 0.855.. Validation loss: 1.712.. \n",
      "Epoch 8/20.. Train loss: 0.567.. Validation loss: 1.733.. \n",
      "Epoch 8/20.. Train loss: 0.248.. Validation loss: 1.748.. \n",
      "Epoch 8/20.. Train loss: 0.583.. Validation loss: 1.697.. \n",
      "Epoch 8/20.. Train loss: 0.562.. Validation loss: 1.718.. \n",
      "Epoch 8/20.. Train loss: 0.234.. Validation loss: 1.682.. \n",
      "Epoch 8/20.. Train loss: 2.888.. Validation loss: 1.711.. \n",
      "Epoch 8/20.. Train loss: 3.694.. Validation loss: 1.55.. \n",
      "Epoch 8/20.. Train loss: 0.071.. Validation loss: 1.56.. \n",
      "Epoch 8/20.. Train loss: 0.205.. Validation loss: 1.544.. \n",
      "Epoch 8/20.. Train loss: 0.108.. Validation loss: 1.558.. \n",
      "Epoch 8/20.. Train loss: 3.526.. Validation loss: 1.541.. \n",
      "Epoch 8/20.. Train loss: 0.274.. Validation loss: 1.557.. \n",
      "Epoch 8/20.. Train loss: 0.112.. Validation loss: 1.563.. \n",
      "Epoch 8/20.. Train loss: 0.067.. Validation loss: 1.57.. \n",
      "Epoch 8/20.. Train loss: 0.009.. Validation loss: 1.574.. \n",
      "Epoch 8/20.. Train loss: 0.000.. Validation loss: 1.574.. \n",
      "Epoch 8/20.. Train loss: 0.028.. Validation loss: 1.567.. \n",
      "Epoch 8/20.. Train loss: 0.555.. Validation loss: 1.584.. \n",
      "Epoch 8/20.. Train loss: 0.251.. Validation loss: 1.598.. \n",
      "Epoch 8/20.. Train loss: 0.488.. Validation loss: 1.568.. \n",
      "Epoch 8/20.. Train loss: 24.259.. Validation loss: 1.828.. \n",
      "Epoch 8/20.. Train loss: 0.988.. Validation loss: 1.731.. \n",
      "Epoch 8/20.. Train loss: 0.547.. Validation loss: 1.776.. \n",
      "Epoch 8/20.. Train loss: 0.637.. Validation loss: 1.725.. \n",
      "Epoch 8/20.. Train loss: 0.434.. Validation loss: 1.768.. \n",
      "Epoch 8/20.. Train loss: 1.777.. Validation loss: 1.878.. \n",
      "Epoch 8/20.. Train loss: 0.067.. Validation loss: 1.904.. \n",
      "Epoch 8/20.. Train loss: 0.089.. Validation loss: 1.886.. \n",
      "Epoch 8/20.. Train loss: 0.009.. Validation loss: 1.889.. \n",
      "Epoch 8/20.. Train loss: 0.641.. Validation loss: 1.832.. \n",
      "Epoch 8/20.. Train loss: 0.274.. Validation loss: 1.795.. \n",
      "Epoch 8/20.. Train loss: 8.606.. Validation loss: 1.543.. \n",
      "Epoch 8/20.. Train loss: 0.405.. Validation loss: 1.526.. \n",
      "Epoch 8/20.. Train loss: 1.741.. Validation loss: 1.557.. \n",
      "Epoch 8/20.. Train loss: 0.449.. Validation loss: 1.585.. \n",
      "Epoch 8/20.. Train loss: 0.433.. Validation loss: 1.617.. \n",
      "Epoch 8/20.. Train loss: 0.721.. Validation loss: 1.636.. \n",
      "Epoch 8/20.. Train loss: 2.818.. Validation loss: 1.747.. \n",
      "Epoch 8/20.. Train loss: 0.086.. Validation loss: 1.767.. \n",
      "Epoch 8/20.. Train loss: 1.105.. Validation loss: 1.836.. \n",
      "Epoch 8/20.. Train loss: 0.199.. Validation loss: 1.883.. \n",
      "Epoch 8/20.. Train loss: 0.659.. Validation loss: 1.945.. \n",
      "Epoch 8/20.. Train loss: 0.353.. Validation loss: 2.006.. \n",
      "Epoch 8/20.. Train loss: 0.008.. Validation loss: 2.015.. \n",
      "Epoch 8/20.. Train loss: 0.042.. Validation loss: 2.007.. \n",
      "Epoch 8/20.. Train loss: 1.683.. Validation loss: 2.137.. \n",
      "Epoch 8/20.. Train loss: 0.069.. Validation loss: 2.175.. \n",
      "Epoch 8/20.. Train loss: 1.859.. Validation loss: 2.056.. \n",
      "Epoch 8/20.. Train loss: 0.466.. Validation loss: 2.109.. \n",
      "Epoch 8/20.. Train loss: 0.082.. Validation loss: 2.15.. \n",
      "Epoch 8/20.. Train loss: 1.089.. Validation loss: 2.081.. \n",
      "Epoch 8/20.. Train loss: 0.686.. Validation loss: 2.022.. \n",
      "Epoch 8/20.. Train loss: 48.644.. Validation loss: 3.23.. \n",
      "Epoch 8/20.. Train loss: 2.126.. Validation loss: 2.973.. \n",
      "Epoch 8/20.. Train loss: 0.055.. Validation loss: 3.009.. \n",
      "Epoch 8/20.. Train loss: 3.595.. Validation loss: 2.644.. \n",
      "Epoch 8/20.. Train loss: 1.963.. Validation loss: 2.587.. \n",
      "Epoch 8/20.. Train loss: 0.675.. Validation loss: 2.688.. \n",
      "Epoch 8/20.. Train loss: 12.637.. Validation loss: 1.761.. \n",
      "Epoch 8/20.. Train loss: 1.199.. Validation loss: 1.852.. \n",
      "Epoch 8/20.. Train loss: 0.048.. Validation loss: 1.869.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20.. Train loss: 0.589.. Validation loss: 1.777.. \n",
      "Epoch 9/20.. Train loss: 0.144.. Validation loss: 1.791.. \n",
      "Epoch 9/20.. Train loss: 0.018.. Validation loss: 1.785.. \n",
      "Epoch 9/20.. Train loss: 0.673.. Validation loss: 1.748.. \n",
      "Epoch 9/20.. Train loss: 2.779.. Validation loss: 1.89.. \n",
      "Epoch 9/20.. Train loss: 0.878.. Validation loss: 1.978.. \n",
      "Epoch 9/20.. Train loss: 0.580.. Validation loss: 2.057.. \n",
      "Epoch 9/20.. Train loss: 0.025.. Validation loss: 2.072.. \n",
      "Epoch 9/20.. Train loss: 0.203.. Validation loss: 2.058.. \n",
      "Epoch 9/20.. Train loss: 1.675.. Validation loss: 1.945.. \n",
      "Epoch 9/20.. Train loss: 1.155.. Validation loss: 1.926.. \n",
      "Epoch 9/20.. Train loss: 0.036.. Validation loss: 1.918.. \n",
      "Epoch 9/20.. Train loss: 0.054.. Validation loss: 1.93.. \n",
      "Epoch 9/20.. Train loss: 0.004.. Validation loss: 1.929.. \n",
      "Epoch 9/20.. Train loss: 1.619.. Validation loss: 1.803.. \n",
      "Epoch 9/20.. Train loss: 0.194.. Validation loss: 1.796.. \n",
      "Epoch 9/20.. Train loss: 0.716.. Validation loss: 1.75.. \n",
      "Epoch 9/20.. Train loss: 0.003.. Validation loss: 1.752.. \n",
      "Epoch 9/20.. Train loss: 0.723.. Validation loss: 1.821.. \n",
      "Epoch 9/20.. Train loss: 0.032.. Validation loss: 1.804.. \n",
      "Epoch 9/20.. Train loss: 0.110.. Validation loss: 1.786.. \n",
      "Epoch 9/20.. Train loss: 0.443.. Validation loss: 1.835.. \n",
      "Epoch 9/20.. Train loss: 0.317.. Validation loss: 1.803.. \n",
      "Epoch 9/20.. Train loss: 2.379.. Validation loss: 1.757.. \n",
      "Epoch 9/20.. Train loss: 1.062.. Validation loss: 1.829.. \n",
      "Epoch 9/20.. Train loss: 4.001.. Validation loss: 1.92.. \n",
      "Epoch 9/20.. Train loss: 9.007.. Validation loss: 1.57.. \n",
      "Epoch 9/20.. Train loss: 2.166.. Validation loss: 1.638.. \n",
      "Epoch 9/20.. Train loss: 0.053.. Validation loss: 1.651.. \n",
      "Epoch 9/20.. Train loss: 1.503.. Validation loss: 1.717.. \n",
      "Epoch 9/20.. Train loss: 0.057.. Validation loss: 1.696.. \n",
      "Epoch 9/20.. Train loss: 0.669.. Validation loss: 1.652.. \n",
      "Epoch 9/20.. Train loss: 0.042.. Validation loss: 1.646.. \n",
      "Epoch 9/20.. Train loss: 0.697.. Validation loss: 1.669.. \n",
      "Epoch 9/20.. Train loss: 0.444.. Validation loss: 1.688.. \n",
      "Epoch 9/20.. Train loss: 0.171.. Validation loss: 1.7.. \n",
      "Epoch 9/20.. Train loss: 0.553.. Validation loss: 1.651.. \n",
      "Epoch 9/20.. Train loss: 0.444.. Validation loss: 1.67.. \n",
      "Epoch 9/20.. Train loss: 0.203.. Validation loss: 1.637.. \n",
      "Epoch 9/20.. Train loss: 2.772.. Validation loss: 1.666.. \n",
      "Epoch 9/20.. Train loss: 3.555.. Validation loss: 1.509.. \n",
      "Epoch 9/20.. Train loss: 0.069.. Validation loss: 1.518.. \n",
      "Epoch 9/20.. Train loss: 0.193.. Validation loss: 1.503.. \n",
      "Epoch 9/20.. Train loss: 0.149.. Validation loss: 1.52.. \n",
      "Epoch 9/20.. Train loss: 3.625.. Validation loss: 1.501.. \n",
      "Epoch 9/20.. Train loss: 0.247.. Validation loss: 1.516.. \n",
      "Epoch 9/20.. Train loss: 0.113.. Validation loss: 1.522.. \n",
      "Epoch 9/20.. Train loss: 0.044.. Validation loss: 1.528.. \n",
      "Epoch 9/20.. Train loss: 0.006.. Validation loss: 1.531.. \n",
      "Epoch 9/20.. Train loss: 0.000.. Validation loss: 1.531.. \n",
      "Epoch 9/20.. Train loss: 0.020.. Validation loss: 1.525.. \n",
      "Epoch 9/20.. Train loss: 0.381.. Validation loss: 1.54.. \n",
      "Epoch 9/20.. Train loss: 0.179.. Validation loss: 1.552.. \n",
      "Epoch 9/20.. Train loss: 0.455.. Validation loss: 1.523.. \n",
      "Epoch 9/20.. Train loss: 24.242.. Validation loss: 1.779.. \n",
      "Epoch 9/20.. Train loss: 0.925.. Validation loss: 1.686.. \n",
      "Epoch 9/20.. Train loss: 0.529.. Validation loss: 1.73.. \n",
      "Epoch 9/20.. Train loss: 0.643.. Validation loss: 1.68.. \n",
      "Epoch 9/20.. Train loss: 0.426.. Validation loss: 1.722.. \n",
      "Epoch 9/20.. Train loss: 1.781.. Validation loss: 1.83.. \n",
      "Epoch 9/20.. Train loss: 0.086.. Validation loss: 1.859.. \n",
      "Epoch 9/20.. Train loss: 0.111.. Validation loss: 1.839.. \n",
      "Epoch 9/20.. Train loss: 0.002.. Validation loss: 1.841.. \n",
      "Epoch 9/20.. Train loss: 0.655.. Validation loss: 1.784.. \n",
      "Epoch 9/20.. Train loss: 0.274.. Validation loss: 1.747.. \n",
      "Epoch 9/20.. Train loss: 8.234.. Validation loss: 1.504.. \n",
      "Epoch 9/20.. Train loss: 0.385.. Validation loss: 1.487.. \n",
      "Epoch 9/20.. Train loss: 1.683.. Validation loss: 1.518.. \n",
      "Epoch 9/20.. Train loss: 0.445.. Validation loss: 1.545.. \n",
      "Epoch 9/20.. Train loss: 0.451.. Validation loss: 1.578.. \n",
      "Epoch 9/20.. Train loss: 0.493.. Validation loss: 1.594.. \n",
      "Epoch 9/20.. Train loss: 2.845.. Validation loss: 1.705.. \n",
      "Epoch 9/20.. Train loss: 0.085.. Validation loss: 1.724.. \n",
      "Epoch 9/20.. Train loss: 1.111.. Validation loss: 1.793.. \n",
      "Epoch 9/20.. Train loss: 0.224.. Validation loss: 1.842.. \n",
      "Epoch 9/20.. Train loss: 0.662.. Validation loss: 1.904.. \n",
      "Epoch 9/20.. Train loss: 0.346.. Validation loss: 1.965.. \n",
      "Epoch 9/20.. Train loss: 0.008.. Validation loss: 1.974.. \n",
      "Epoch 9/20.. Train loss: 0.114.. Validation loss: 1.96.. \n",
      "Epoch 9/20.. Train loss: 1.692.. Validation loss: 2.089.. \n",
      "Epoch 9/20.. Train loss: 0.087.. Validation loss: 2.131.. \n",
      "Epoch 9/20.. Train loss: 1.952.. Validation loss: 2.01.. \n",
      "Epoch 9/20.. Train loss: 0.434.. Validation loss: 2.061.. \n",
      "Epoch 9/20.. Train loss: 0.102.. Validation loss: 2.106.. \n",
      "Epoch 9/20.. Train loss: 1.150.. Validation loss: 2.035.. \n",
      "Epoch 9/20.. Train loss: 0.711.. Validation loss: 1.975.. \n",
      "Epoch 9/20.. Train loss: 49.237.. Validation loss: 3.184.. \n",
      "Epoch 9/20.. Train loss: 2.174.. Validation loss: 2.924.. \n",
      "Epoch 9/20.. Train loss: 0.053.. Validation loss: 2.959.. \n",
      "Epoch 9/20.. Train loss: 3.375.. Validation loss: 2.607.. \n",
      "Epoch 9/20.. Train loss: 1.982.. Validation loss: 2.549.. \n",
      "Epoch 9/20.. Train loss: 0.669.. Validation loss: 2.65.. \n",
      "Epoch 9/20.. Train loss: 11.605.. Validation loss: 1.753.. \n",
      "Epoch 9/20.. Train loss: 1.136.. Validation loss: 1.844.. \n",
      "Epoch 9/20.. Train loss: 0.036.. Validation loss: 1.86.. \n",
      "Epoch 9/20.. Train loss: 0.538.. Validation loss: 1.77.. \n",
      "Epoch 10/20.. Train loss: 0.086.. Validation loss: 1.781.. \n",
      "Epoch 10/20.. Train loss: 0.078.. Validation loss: 1.768.. \n",
      "Epoch 10/20.. Train loss: 0.739.. Validation loss: 1.728.. \n",
      "Epoch 10/20.. Train loss: 2.733.. Validation loss: 1.872.. \n",
      "Epoch 10/20.. Train loss: 0.814.. Validation loss: 1.958.. \n",
      "Epoch 10/20.. Train loss: 0.534.. Validation loss: 2.034.. \n",
      "Epoch 10/20.. Train loss: 0.019.. Validation loss: 2.047.. \n",
      "Epoch 10/20.. Train loss: 0.300.. Validation loss: 2.03.. \n",
      "Epoch 10/20.. Train loss: 1.738.. Validation loss: 1.914.. \n",
      "Epoch 10/20.. Train loss: 1.282.. Validation loss: 1.894.. \n",
      "Epoch 10/20.. Train loss: 0.041.. Validation loss: 1.885.. \n",
      "Epoch 10/20.. Train loss: 0.050.. Validation loss: 1.896.. \n",
      "Epoch 10/20.. Train loss: 0.014.. Validation loss: 1.894.. \n",
      "Epoch 10/20.. Train loss: 1.555.. Validation loss: 1.771.. \n",
      "Epoch 10/20.. Train loss: 0.300.. Validation loss: 1.762.. \n",
      "Epoch 10/20.. Train loss: 0.778.. Validation loss: 1.715.. \n",
      "Epoch 10/20.. Train loss: 0.003.. Validation loss: 1.713.. \n",
      "Epoch 10/20.. Train loss: 0.719.. Validation loss: 1.782.. \n",
      "Epoch 10/20.. Train loss: 0.021.. Validation loss: 1.768.. \n",
      "Epoch 10/20.. Train loss: 0.127.. Validation loss: 1.749.. \n",
      "Epoch 10/20.. Train loss: 0.436.. Validation loss: 1.797.. \n",
      "Epoch 10/20.. Train loss: 0.354.. Validation loss: 1.763.. \n",
      "Epoch 10/20.. Train loss: 2.374.. Validation loss: 1.717.. \n",
      "Epoch 10/20.. Train loss: 1.049.. Validation loss: 1.788.. \n",
      "Epoch 10/20.. Train loss: 3.941.. Validation loss: 1.878.. \n",
      "Epoch 10/20.. Train loss: 8.471.. Validation loss: 1.542.. \n",
      "Epoch 10/20.. Train loss: 2.136.. Validation loss: 1.609.. \n",
      "Epoch 10/20.. Train loss: 0.051.. Validation loss: 1.623.. \n",
      "Epoch 10/20.. Train loss: 1.498.. Validation loss: 1.688.. \n",
      "Epoch 10/20.. Train loss: 0.044.. Validation loss: 1.67.. \n",
      "Epoch 10/20.. Train loss: 0.637.. Validation loss: 1.627.. \n",
      "Epoch 10/20.. Train loss: 0.055.. Validation loss: 1.62.. \n",
      "Epoch 10/20.. Train loss: 0.586.. Validation loss: 1.641.. \n",
      "Epoch 10/20.. Train loss: 0.361.. Validation loss: 1.659.. \n",
      "Epoch 10/20.. Train loss: 0.122.. Validation loss: 1.669.. \n",
      "Epoch 10/20.. Train loss: 0.533.. Validation loss: 1.621.. \n",
      "Epoch 10/20.. Train loss: 0.364.. Validation loss: 1.638.. \n",
      "Epoch 10/20.. Train loss: 0.183.. Validation loss: 1.608.. \n",
      "Epoch 10/20.. Train loss: 2.701.. Validation loss: 1.636.. \n",
      "Epoch 10/20.. Train loss: 3.465.. Validation loss: 1.482.. \n",
      "Epoch 10/20.. Train loss: 0.067.. Validation loss: 1.491.. \n",
      "Epoch 10/20.. Train loss: 0.185.. Validation loss: 1.476.. \n",
      "Epoch 10/20.. Train loss: 0.183.. Validation loss: 1.495.. \n",
      "Epoch 10/20.. Train loss: 3.644.. Validation loss: 1.475.. \n",
      "Epoch 10/20.. Train loss: 0.225.. Validation loss: 1.489.. \n",
      "Epoch 10/20.. Train loss: 0.117.. Validation loss: 1.496.. \n",
      "Epoch 10/20.. Train loss: 0.029.. Validation loss: 1.501.. \n",
      "Epoch 10/20.. Train loss: 0.004.. Validation loss: 1.504.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20.. Train loss: 0.000.. Validation loss: 1.503.. \n",
      "Epoch 10/20.. Train loss: 0.015.. Validation loss: 1.498.. \n",
      "Epoch 10/20.. Train loss: 0.266.. Validation loss: 1.51.. \n",
      "Epoch 10/20.. Train loss: 0.132.. Validation loss: 1.52.. \n",
      "Epoch 10/20.. Train loss: 0.430.. Validation loss: 1.493.. \n",
      "Epoch 10/20.. Train loss: 24.240.. Validation loss: 1.745.. \n",
      "Epoch 10/20.. Train loss: 0.882.. Validation loss: 1.656.. \n",
      "Epoch 10/20.. Train loss: 0.518.. Validation loss: 1.699.. \n",
      "Epoch 10/20.. Train loss: 0.644.. Validation loss: 1.649.. \n",
      "Epoch 10/20.. Train loss: 0.422.. Validation loss: 1.691.. \n",
      "Epoch 10/20.. Train loss: 1.779.. Validation loss: 1.798.. \n",
      "Epoch 10/20.. Train loss: 0.103.. Validation loss: 1.83.. \n",
      "Epoch 10/20.. Train loss: 0.128.. Validation loss: 1.808.. \n",
      "Epoch 10/20.. Train loss: 0.000.. Validation loss: 1.808.. \n",
      "Epoch 10/20.. Train loss: 0.662.. Validation loss: 1.752.. \n",
      "Epoch 10/20.. Train loss: 0.274.. Validation loss: 1.715.. \n",
      "Epoch 10/20.. Train loss: 7.959.. Validation loss: 1.479.. \n",
      "Epoch 10/20.. Train loss: 0.366.. Validation loss: 1.462.. \n",
      "Epoch 10/20.. Train loss: 1.636.. Validation loss: 1.493.. \n",
      "Epoch 10/20.. Train loss: 0.440.. Validation loss: 1.52.. \n",
      "Epoch 10/20.. Train loss: 0.463.. Validation loss: 1.553.. \n",
      "Epoch 10/20.. Train loss: 0.345.. Validation loss: 1.567.. \n",
      "Epoch 10/20.. Train loss: 2.862.. Validation loss: 1.676.. \n",
      "Epoch 10/20.. Train loss: 0.085.. Validation loss: 1.696.. \n",
      "Epoch 10/20.. Train loss: 1.121.. Validation loss: 1.764.. \n",
      "Epoch 10/20.. Train loss: 0.243.. Validation loss: 1.815.. \n",
      "Epoch 10/20.. Train loss: 0.670.. Validation loss: 1.877.. \n",
      "Epoch 10/20.. Train loss: 0.339.. Validation loss: 1.937.. \n",
      "Epoch 10/20.. Train loss: 0.007.. Validation loss: 1.946.. \n",
      "Epoch 10/20.. Train loss: 0.194.. Validation loss: 1.927.. \n",
      "Epoch 10/20.. Train loss: 1.695.. Validation loss: 2.055.. \n",
      "Epoch 10/20.. Train loss: 0.101.. Validation loss: 2.101.. \n",
      "Epoch 10/20.. Train loss: 2.025.. Validation loss: 1.978.. \n",
      "Epoch 10/20.. Train loss: 0.409.. Validation loss: 2.027.. \n",
      "Epoch 10/20.. Train loss: 0.117.. Validation loss: 2.075.. \n",
      "Epoch 10/20.. Train loss: 1.183.. Validation loss: 2.003.. \n",
      "Epoch 10/20.. Train loss: 0.721.. Validation loss: 1.943.. \n",
      "Epoch 10/20.. Train loss: 49.679.. Validation loss: 3.151.. \n",
      "Epoch 10/20.. Train loss: 2.216.. Validation loss: 2.889.. \n",
      "Epoch 10/20.. Train loss: 0.051.. Validation loss: 2.924.. \n",
      "Epoch 10/20.. Train loss: 3.217.. Validation loss: 2.581.. \n",
      "Epoch 10/20.. Train loss: 1.966.. Validation loss: 2.524.. \n",
      "Epoch 10/20.. Train loss: 0.667.. Validation loss: 2.624.. \n",
      "Epoch 10/20.. Train loss: 10.871.. Validation loss: 1.75.. \n",
      "Epoch 10/20.. Train loss: 1.088.. Validation loss: 1.842.. \n",
      "Epoch 10/20.. Train loss: 0.028.. Validation loss: 1.856.. \n",
      "Epoch 10/20.. Train loss: 0.502.. Validation loss: 1.767.. \n",
      "Epoch 11/20.. Train loss: 0.052.. Validation loss: 1.776.. \n",
      "Epoch 11/20.. Train loss: 0.152.. Validation loss: 1.758.. \n",
      "Epoch 11/20.. Train loss: 0.780.. Validation loss: 1.715.. \n",
      "Epoch 11/20.. Train loss: 2.692.. Validation loss: 1.86.. \n",
      "Epoch 11/20.. Train loss: 0.768.. Validation loss: 1.944.. \n",
      "Epoch 11/20.. Train loss: 0.500.. Validation loss: 2.018.. \n",
      "Epoch 11/20.. Train loss: 0.015.. Validation loss: 2.031.. \n",
      "Epoch 11/20.. Train loss: 0.384.. Validation loss: 2.01.. \n",
      "Epoch 11/20.. Train loss: 1.778.. Validation loss: 1.893.. \n",
      "Epoch 11/20.. Train loss: 1.369.. Validation loss: 1.871.. \n",
      "Epoch 11/20.. Train loss: 0.042.. Validation loss: 1.862.. \n",
      "Epoch 11/20.. Train loss: 0.049.. Validation loss: 1.873.. \n",
      "Epoch 11/20.. Train loss: 0.024.. Validation loss: 1.872.. \n",
      "Epoch 11/20.. Train loss: 1.506.. Validation loss: 1.751.. \n",
      "Epoch 11/20.. Train loss: 0.394.. Validation loss: 1.739.. \n",
      "Epoch 11/20.. Train loss: 0.825.. Validation loss: 1.691.. \n",
      "Epoch 11/20.. Train loss: 0.018.. Validation loss: 1.687.. \n",
      "Epoch 11/20.. Train loss: 0.714.. Validation loss: 1.755.. \n",
      "Epoch 11/20.. Train loss: 0.014.. Validation loss: 1.743.. \n",
      "Epoch 11/20.. Train loss: 0.140.. Validation loss: 1.724.. \n",
      "Epoch 11/20.. Train loss: 0.430.. Validation loss: 1.771.. \n",
      "Epoch 11/20.. Train loss: 0.381.. Validation loss: 1.736.. \n",
      "Epoch 11/20.. Train loss: 2.337.. Validation loss: 1.691.. \n",
      "Epoch 11/20.. Train loss: 1.033.. Validation loss: 1.76.. \n",
      "Epoch 11/20.. Train loss: 3.897.. Validation loss: 1.849.. \n",
      "Epoch 11/20.. Train loss: 8.087.. Validation loss: 1.523.. \n",
      "Epoch 11/20.. Train loss: 2.111.. Validation loss: 1.591.. \n",
      "Epoch 11/20.. Train loss: 0.049.. Validation loss: 1.604.. \n",
      "Epoch 11/20.. Train loss: 1.497.. Validation loss: 1.67.. \n",
      "Epoch 11/20.. Train loss: 0.036.. Validation loss: 1.653.. \n",
      "Epoch 11/20.. Train loss: 0.606.. Validation loss: 1.61.. \n",
      "Epoch 11/20.. Train loss: 0.063.. Validation loss: 1.603.. \n",
      "Epoch 11/20.. Train loss: 0.507.. Validation loss: 1.623.. \n",
      "Epoch 11/20.. Train loss: 0.303.. Validation loss: 1.639.. \n",
      "Epoch 11/20.. Train loss: 0.090.. Validation loss: 1.649.. \n",
      "Epoch 11/20.. Train loss: 0.518.. Validation loss: 1.601.. \n",
      "Epoch 11/20.. Train loss: 0.308.. Validation loss: 1.617.. \n",
      "Epoch 11/20.. Train loss: 0.169.. Validation loss: 1.588.. \n",
      "Epoch 11/20.. Train loss: 2.661.. Validation loss: 1.617.. \n",
      "Epoch 11/20.. Train loss: 3.408.. Validation loss: 1.464.. \n",
      "Epoch 11/20.. Train loss: 0.065.. Validation loss: 1.473.. \n",
      "Epoch 11/20.. Train loss: 0.179.. Validation loss: 1.458.. \n",
      "Epoch 11/20.. Train loss: 0.210.. Validation loss: 1.478.. \n",
      "Epoch 11/20.. Train loss: 3.608.. Validation loss: 1.458.. \n",
      "Epoch 11/20.. Train loss: 0.208.. Validation loss: 1.472.. \n",
      "Epoch 11/20.. Train loss: 0.124.. Validation loss: 1.479.. \n",
      "Epoch 11/20.. Train loss: 0.020.. Validation loss: 1.483.. \n",
      "Epoch 11/20.. Train loss: 0.003.. Validation loss: 1.485.. \n",
      "Epoch 11/20.. Train loss: 0.001.. Validation loss: 1.484.. \n",
      "Epoch 11/20.. Train loss: 0.012.. Validation loss: 1.479.. \n",
      "Epoch 11/20.. Train loss: 0.189.. Validation loss: 1.49.. \n",
      "Epoch 11/20.. Train loss: 0.102.. Validation loss: 1.498.. \n",
      "Epoch 11/20.. Train loss: 0.412.. Validation loss: 1.473.. \n",
      "Epoch 11/20.. Train loss: 24.247.. Validation loss: 1.722.. \n",
      "Epoch 11/20.. Train loss: 0.852.. Validation loss: 1.635.. \n",
      "Epoch 11/20.. Train loss: 0.513.. Validation loss: 1.677.. \n",
      "Epoch 11/20.. Train loss: 0.643.. Validation loss: 1.628.. \n",
      "Epoch 11/20.. Train loss: 0.420.. Validation loss: 1.669.. \n",
      "Epoch 11/20.. Train loss: 1.773.. Validation loss: 1.775.. \n",
      "Epoch 11/20.. Train loss: 0.117.. Validation loss: 1.809.. \n",
      "Epoch 11/20.. Train loss: 0.139.. Validation loss: 1.786.. \n",
      "Epoch 11/20.. Train loss: 0.000.. Validation loss: 1.786.. \n",
      "Epoch 11/20.. Train loss: 0.666.. Validation loss: 1.729.. \n",
      "Epoch 11/20.. Train loss: 0.274.. Validation loss: 1.693.. \n",
      "Epoch 11/20.. Train loss: 7.753.. Validation loss: 1.462.. \n",
      "Epoch 11/20.. Train loss: 0.348.. Validation loss: 1.446.. \n",
      "Epoch 11/20.. Train loss: 1.597.. Validation loss: 1.476.. \n",
      "Epoch 11/20.. Train loss: 0.434.. Validation loss: 1.503.. \n",
      "Epoch 11/20.. Train loss: 0.469.. Validation loss: 1.536.. \n",
      "Epoch 11/20.. Train loss: 0.246.. Validation loss: 1.548.. \n",
      "Epoch 11/20.. Train loss: 2.872.. Validation loss: 1.656.. \n",
      "Epoch 11/20.. Train loss: 0.086.. Validation loss: 1.676.. \n",
      "Epoch 11/20.. Train loss: 1.133.. Validation loss: 1.744.. \n",
      "Epoch 11/20.. Train loss: 0.256.. Validation loss: 1.797.. \n",
      "Epoch 11/20.. Train loss: 0.680.. Validation loss: 1.859.. \n",
      "Epoch 11/20.. Train loss: 0.332.. Validation loss: 1.918.. \n",
      "Epoch 11/20.. Train loss: 0.006.. Validation loss: 1.926.. \n",
      "Epoch 11/20.. Train loss: 0.270.. Validation loss: 1.904.. \n",
      "Epoch 11/20.. Train loss: 1.694.. Validation loss: 2.032.. \n",
      "Epoch 11/20.. Train loss: 0.112.. Validation loss: 2.079.. \n",
      "Epoch 11/20.. Train loss: 2.081.. Validation loss: 1.955.. \n",
      "Epoch 11/20.. Train loss: 0.389.. Validation loss: 2.003.. \n",
      "Epoch 11/20.. Train loss: 0.129.. Validation loss: 2.052.. \n",
      "Epoch 11/20.. Train loss: 1.196.. Validation loss: 1.98.. \n",
      "Epoch 11/20.. Train loss: 0.720.. Validation loss: 1.92.. \n",
      "Epoch 11/20.. Train loss: 50.011.. Validation loss: 3.128.. \n",
      "Epoch 11/20.. Train loss: 2.253.. Validation loss: 2.865.. \n",
      "Epoch 11/20.. Train loss: 0.049.. Validation loss: 2.898.. \n",
      "Epoch 11/20.. Train loss: 3.103.. Validation loss: 2.563.. \n",
      "Epoch 11/20.. Train loss: 1.926.. Validation loss: 2.505.. \n",
      "Epoch 11/20.. Train loss: 0.667.. Validation loss: 2.606.. \n",
      "Epoch 11/20.. Train loss: 10.341.. Validation loss: 1.749.. \n",
      "Epoch 11/20.. Train loss: 1.052.. Validation loss: 1.841.. \n",
      "Epoch 11/20.. Train loss: 0.024.. Validation loss: 1.855.. \n",
      "Epoch 11/20.. Train loss: 0.478.. Validation loss: 1.767.. \n",
      "Epoch 12/20.. Train loss: 0.032.. Validation loss: 1.774.. \n",
      "Epoch 12/20.. Train loss: 0.228.. Validation loss: 1.751.. \n",
      "Epoch 12/20.. Train loss: 0.802.. Validation loss: 1.707.. \n",
      "Epoch 12/20.. Train loss: 2.656.. Validation loss: 1.852.. \n",
      "Epoch 12/20.. Train loss: 0.733.. Validation loss: 1.935.. \n",
      "Epoch 12/20.. Train loss: 0.475.. Validation loss: 2.007.. \n",
      "Epoch 12/20.. Train loss: 0.013.. Validation loss: 2.019.. \n",
      "Epoch 12/20.. Train loss: 0.451.. Validation loss: 1.996.. \n",
      "Epoch 12/20.. Train loss: 1.802.. Validation loss: 1.878.. \n",
      "Epoch 12/20.. Train loss: 1.425.. Validation loss: 1.856.. \n",
      "Epoch 12/20.. Train loss: 0.039.. Validation loss: 1.847.. \n",
      "Epoch 12/20.. Train loss: 0.051.. Validation loss: 1.858.. \n",
      "Epoch 12/20.. Train loss: 0.034.. Validation loss: 1.856.. \n",
      "Epoch 12/20.. Train loss: 1.469.. Validation loss: 1.736.. \n",
      "Epoch 12/20.. Train loss: 0.474.. Validation loss: 1.724.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20.. Train loss: 0.858.. Validation loss: 1.674.. \n",
      "Epoch 12/20.. Train loss: 0.040.. Validation loss: 1.669.. \n",
      "Epoch 12/20.. Train loss: 0.706.. Validation loss: 1.736.. \n",
      "Epoch 12/20.. Train loss: 0.010.. Validation loss: 1.726.. \n",
      "Epoch 12/20.. Train loss: 0.148.. Validation loss: 1.706.. \n",
      "Epoch 12/20.. Train loss: 0.427.. Validation loss: 1.753.. \n",
      "Epoch 12/20.. Train loss: 0.401.. Validation loss: 1.717.. \n",
      "Epoch 12/20.. Train loss: 2.278.. Validation loss: 1.673.. \n",
      "Epoch 12/20.. Train loss: 1.016.. Validation loss: 1.741.. \n",
      "Epoch 12/20.. Train loss: 3.863.. Validation loss: 1.829.. \n",
      "Epoch 12/20.. Train loss: 7.808.. Validation loss: 1.51.. \n",
      "Epoch 12/20.. Train loss: 2.090.. Validation loss: 1.578.. \n",
      "Epoch 12/20.. Train loss: 0.047.. Validation loss: 1.591.. \n",
      "Epoch 12/20.. Train loss: 1.501.. Validation loss: 1.657.. \n",
      "Epoch 12/20.. Train loss: 0.032.. Validation loss: 1.641.. \n",
      "Epoch 12/20.. Train loss: 0.578.. Validation loss: 1.599.. \n",
      "Epoch 12/20.. Train loss: 0.068.. Validation loss: 1.592.. \n",
      "Epoch 12/20.. Train loss: 0.448.. Validation loss: 1.611.. \n",
      "Epoch 12/20.. Train loss: 0.261.. Validation loss: 1.626.. \n",
      "Epoch 12/20.. Train loss: 0.068.. Validation loss: 1.634.. \n",
      "Epoch 12/20.. Train loss: 0.507.. Validation loss: 1.588.. \n",
      "Epoch 12/20.. Train loss: 0.268.. Validation loss: 1.602.. \n",
      "Epoch 12/20.. Train loss: 0.160.. Validation loss: 1.574.. \n",
      "Epoch 12/20.. Train loss: 2.643.. Validation loss: 1.603.. \n",
      "Epoch 12/20.. Train loss: 3.375.. Validation loss: 1.452.. \n",
      "Epoch 12/20.. Train loss: 0.063.. Validation loss: 1.461.. \n",
      "Epoch 12/20.. Train loss: 0.175.. Validation loss: 1.446.. \n",
      "Epoch 12/20.. Train loss: 0.231.. Validation loss: 1.467.. \n",
      "Epoch 12/20.. Train loss: 3.533.. Validation loss: 1.446.. \n",
      "Epoch 12/20.. Train loss: 0.193.. Validation loss: 1.459.. \n",
      "Epoch 12/20.. Train loss: 0.132.. Validation loss: 1.467.. \n",
      "Epoch 12/20.. Train loss: 0.014.. Validation loss: 1.47.. \n",
      "Epoch 12/20.. Train loss: 0.002.. Validation loss: 1.472.. \n",
      "Epoch 12/20.. Train loss: 0.001.. Validation loss: 1.471.. \n",
      "Epoch 12/20.. Train loss: 0.010.. Validation loss: 1.466.. \n",
      "Epoch 12/20.. Train loss: 0.136.. Validation loss: 1.475.. \n",
      "Epoch 12/20.. Train loss: 0.081.. Validation loss: 1.483.. \n",
      "Epoch 12/20.. Train loss: 0.398.. Validation loss: 1.458.. \n",
      "Epoch 12/20.. Train loss: 24.260.. Validation loss: 1.704.. \n",
      "Epoch 12/20.. Train loss: 0.831.. Validation loss: 1.619.. \n",
      "Epoch 12/20.. Train loss: 0.512.. Validation loss: 1.661.. \n",
      "Epoch 12/20.. Train loss: 0.640.. Validation loss: 1.612.. \n",
      "Epoch 12/20.. Train loss: 0.421.. Validation loss: 1.653.. \n",
      "Epoch 12/20.. Train loss: 1.765.. Validation loss: 1.758.. \n",
      "Epoch 12/20.. Train loss: 0.128.. Validation loss: 1.793.. \n",
      "Epoch 12/20.. Train loss: 0.146.. Validation loss: 1.771.. \n",
      "Epoch 12/20.. Train loss: 0.001.. Validation loss: 1.769.. \n",
      "Epoch 12/20.. Train loss: 0.666.. Validation loss: 1.713.. \n",
      "Epoch 12/20.. Train loss: 0.273.. Validation loss: 1.677.. \n",
      "Epoch 12/20.. Train loss: 7.595.. Validation loss: 1.45.. \n",
      "Epoch 12/20.. Train loss: 0.330.. Validation loss: 1.434.. \n",
      "Epoch 12/20.. Train loss: 1.564.. Validation loss: 1.464.. \n",
      "Epoch 12/20.. Train loss: 0.428.. Validation loss: 1.491.. \n",
      "Epoch 12/20.. Train loss: 0.471.. Validation loss: 1.524.. \n",
      "Epoch 12/20.. Train loss: 0.179.. Validation loss: 1.534.. \n",
      "Epoch 12/20.. Train loss: 2.878.. Validation loss: 1.642.. \n",
      "Epoch 12/20.. Train loss: 0.088.. Validation loss: 1.662.. \n",
      "Epoch 12/20.. Train loss: 1.146.. Validation loss: 1.73.. \n",
      "Epoch 12/20.. Train loss: 0.266.. Validation loss: 1.783.. \n",
      "Epoch 12/20.. Train loss: 0.692.. Validation loss: 1.846.. \n",
      "Epoch 12/20.. Train loss: 0.326.. Validation loss: 1.905.. \n",
      "Epoch 12/20.. Train loss: 0.006.. Validation loss: 1.912.. \n",
      "Epoch 12/20.. Train loss: 0.339.. Validation loss: 1.888.. \n",
      "Epoch 12/20.. Train loss: 1.690.. Validation loss: 2.014.. \n",
      "Epoch 12/20.. Train loss: 0.119.. Validation loss: 2.063.. \n",
      "Epoch 12/20.. Train loss: 2.125.. Validation loss: 1.938.. \n",
      "Epoch 12/20.. Train loss: 0.373.. Validation loss: 1.984.. \n",
      "Epoch 12/20.. Train loss: 0.137.. Validation loss: 2.035.. \n",
      "Epoch 12/20.. Train loss: 1.195.. Validation loss: 1.964.. \n",
      "Epoch 12/20.. Train loss: 0.711.. Validation loss: 1.904.. \n",
      "Epoch 12/20.. Train loss: 50.261.. Validation loss: 3.111.. \n",
      "Epoch 12/20.. Train loss: 2.286.. Validation loss: 2.846.. \n",
      "Epoch 12/20.. Train loss: 0.047.. Validation loss: 2.879.. \n",
      "Epoch 12/20.. Train loss: 3.020.. Validation loss: 2.549.. \n",
      "Epoch 12/20.. Train loss: 1.870.. Validation loss: 2.492.. \n",
      "Epoch 12/20.. Train loss: 0.668.. Validation loss: 2.593.. \n",
      "Epoch 12/20.. Train loss: 9.955.. Validation loss: 1.75.. \n",
      "Epoch 12/20.. Train loss: 1.023.. Validation loss: 1.842.. \n",
      "Epoch 12/20.. Train loss: 0.021.. Validation loss: 1.854.. \n",
      "Epoch 12/20.. Train loss: 0.461.. Validation loss: 1.767.. \n",
      "Epoch 13/20.. Train loss: 0.020.. Validation loss: 1.772.. \n",
      "Epoch 13/20.. Train loss: 0.301.. Validation loss: 1.745.. \n",
      "Epoch 13/20.. Train loss: 0.810.. Validation loss: 1.701.. \n",
      "Epoch 13/20.. Train loss: 2.624.. Validation loss: 1.846.. \n",
      "Epoch 13/20.. Train loss: 0.706.. Validation loss: 1.928.. \n",
      "Epoch 13/20.. Train loss: 0.457.. Validation loss: 1.999.. \n",
      "Epoch 13/20.. Train loss: 0.011.. Validation loss: 2.009.. \n",
      "Epoch 13/20.. Train loss: 0.503.. Validation loss: 1.986.. \n",
      "Epoch 13/20.. Train loss: 1.814.. Validation loss: 1.867.. \n",
      "Epoch 13/20.. Train loss: 1.458.. Validation loss: 1.844.. \n",
      "Epoch 13/20.. Train loss: 0.035.. Validation loss: 1.836.. \n",
      "Epoch 13/20.. Train loss: 0.054.. Validation loss: 1.847.. \n",
      "Epoch 13/20.. Train loss: 0.042.. Validation loss: 1.845.. \n",
      "Epoch 13/20.. Train loss: 1.440.. Validation loss: 1.727.. \n",
      "Epoch 13/20.. Train loss: 0.539.. Validation loss: 1.713.. \n",
      "Epoch 13/20.. Train loss: 0.883.. Validation loss: 1.663.. \n",
      "Epoch 13/20.. Train loss: 0.062.. Validation loss: 1.656.. \n",
      "Epoch 13/20.. Train loss: 0.698.. Validation loss: 1.722.. \n",
      "Epoch 13/20.. Train loss: 0.007.. Validation loss: 1.714.. \n",
      "Epoch 13/20.. Train loss: 0.154.. Validation loss: 1.693.. \n",
      "Epoch 13/20.. Train loss: 0.425.. Validation loss: 1.74.. \n",
      "Epoch 13/20.. Train loss: 0.415.. Validation loss: 1.704.. \n",
      "Epoch 13/20.. Train loss: 2.206.. Validation loss: 1.66.. \n",
      "Epoch 13/20.. Train loss: 0.998.. Validation loss: 1.727.. \n",
      "Epoch 13/20.. Train loss: 3.837.. Validation loss: 1.815.. \n",
      "Epoch 13/20.. Train loss: 7.605.. Validation loss: 1.501.. \n",
      "Epoch 13/20.. Train loss: 2.072.. Validation loss: 1.569.. \n",
      "Epoch 13/20.. Train loss: 0.046.. Validation loss: 1.581.. \n",
      "Epoch 13/20.. Train loss: 1.506.. Validation loss: 1.648.. \n",
      "Epoch 13/20.. Train loss: 0.029.. Validation loss: 1.633.. \n",
      "Epoch 13/20.. Train loss: 0.552.. Validation loss: 1.592.. \n",
      "Epoch 13/20.. Train loss: 0.070.. Validation loss: 1.584.. \n",
      "Epoch 13/20.. Train loss: 0.404.. Validation loss: 1.603.. \n",
      "Epoch 13/20.. Train loss: 0.230.. Validation loss: 1.617.. \n",
      "Epoch 13/20.. Train loss: 0.052.. Validation loss: 1.624.. \n",
      "Epoch 13/20.. Train loss: 0.499.. Validation loss: 1.578.. \n",
      "Epoch 13/20.. Train loss: 0.238.. Validation loss: 1.592.. \n",
      "Epoch 13/20.. Train loss: 0.154.. Validation loss: 1.564.. \n",
      "Epoch 13/20.. Train loss: 2.639.. Validation loss: 1.593.. \n",
      "Epoch 13/20.. Train loss: 3.358.. Validation loss: 1.443.. \n",
      "Epoch 13/20.. Train loss: 0.061.. Validation loss: 1.452.. \n",
      "Epoch 13/20.. Train loss: 0.173.. Validation loss: 1.437.. \n",
      "Epoch 13/20.. Train loss: 0.247.. Validation loss: 1.459.. \n",
      "Epoch 13/20.. Train loss: 3.434.. Validation loss: 1.437.. \n",
      "Epoch 13/20.. Train loss: 0.181.. Validation loss: 1.451.. \n",
      "Epoch 13/20.. Train loss: 0.142.. Validation loss: 1.459.. \n",
      "Epoch 13/20.. Train loss: 0.010.. Validation loss: 1.461.. \n",
      "Epoch 13/20.. Train loss: 0.001.. Validation loss: 1.463.. \n",
      "Epoch 13/20.. Train loss: 0.002.. Validation loss: 1.461.. \n",
      "Epoch 13/20.. Train loss: 0.009.. Validation loss: 1.457.. \n",
      "Epoch 13/20.. Train loss: 0.098.. Validation loss: 1.464.. \n",
      "Epoch 13/20.. Train loss: 0.066.. Validation loss: 1.471.. \n",
      "Epoch 13/20.. Train loss: 0.388.. Validation loss: 1.448.. \n",
      "Epoch 13/20.. Train loss: 24.278.. Validation loss: 1.691.. \n",
      "Epoch 13/20.. Train loss: 0.818.. Validation loss: 1.607.. \n",
      "Epoch 13/20.. Train loss: 0.513.. Validation loss: 1.649.. \n",
      "Epoch 13/20.. Train loss: 0.635.. Validation loss: 1.601.. \n",
      "Epoch 13/20.. Train loss: 0.422.. Validation loss: 1.641.. \n",
      "Epoch 13/20.. Train loss: 1.755.. Validation loss: 1.746.. \n",
      "Epoch 13/20.. Train loss: 0.138.. Validation loss: 1.782.. \n",
      "Epoch 13/20.. Train loss: 0.149.. Validation loss: 1.759.. \n",
      "Epoch 13/20.. Train loss: 0.002.. Validation loss: 1.757.. \n",
      "Epoch 13/20.. Train loss: 0.665.. Validation loss: 1.701.. \n",
      "Epoch 13/20.. Train loss: 0.273.. Validation loss: 1.665.. \n",
      "Epoch 13/20.. Train loss: 7.474.. Validation loss: 1.442.. \n",
      "Epoch 13/20.. Train loss: 0.313.. Validation loss: 1.426.. \n",
      "Epoch 13/20.. Train loss: 1.535.. Validation loss: 1.456.. \n",
      "Epoch 13/20.. Train loss: 0.422.. Validation loss: 1.482.. \n",
      "Epoch 13/20.. Train loss: 0.470.. Validation loss: 1.515.. \n",
      "Epoch 13/20.. Train loss: 0.131.. Validation loss: 1.524.. \n",
      "Epoch 13/20.. Train loss: 2.879.. Validation loss: 1.631.. \n",
      "Epoch 13/20.. Train loss: 0.090.. Validation loss: 1.651.. \n",
      "Epoch 13/20.. Train loss: 1.160.. Validation loss: 1.72.. \n",
      "Epoch 13/20.. Train loss: 0.273.. Validation loss: 1.773.. \n",
      "Epoch 13/20.. Train loss: 0.705.. Validation loss: 1.837.. \n",
      "Epoch 13/20.. Train loss: 0.320.. Validation loss: 1.895.. \n",
      "Epoch 13/20.. Train loss: 0.005.. Validation loss: 1.902.. \n",
      "Epoch 13/20.. Train loss: 0.400.. Validation loss: 1.875.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20.. Train loss: 1.684.. Validation loss: 2.0.. \n",
      "Epoch 13/20.. Train loss: 0.124.. Validation loss: 2.05.. \n",
      "Epoch 13/20.. Train loss: 2.160.. Validation loss: 1.925.. \n",
      "Epoch 13/20.. Train loss: 0.360.. Validation loss: 1.97.. \n",
      "Epoch 13/20.. Train loss: 0.143.. Validation loss: 2.022.. \n",
      "Epoch 13/20.. Train loss: 1.183.. Validation loss: 1.951.. \n",
      "Epoch 13/20.. Train loss: 0.698.. Validation loss: 1.892.. \n",
      "Epoch 13/20.. Train loss: 50.451.. Validation loss: 3.098.. \n",
      "Epoch 13/20.. Train loss: 2.316.. Validation loss: 2.831.. \n",
      "Epoch 13/20.. Train loss: 0.045.. Validation loss: 2.863.. \n",
      "Epoch 13/20.. Train loss: 2.958.. Validation loss: 2.538.. \n",
      "Epoch 13/20.. Train loss: 1.805.. Validation loss: 2.482.. \n",
      "Epoch 13/20.. Train loss: 0.671.. Validation loss: 2.583.. \n",
      "Epoch 13/20.. Train loss: 9.668.. Validation loss: 1.75.. \n",
      "Epoch 13/20.. Train loss: 1.001.. Validation loss: 1.842.. \n",
      "Epoch 13/20.. Train loss: 0.019.. Validation loss: 1.854.. \n",
      "Epoch 13/20.. Train loss: 0.450.. Validation loss: 1.767.. \n",
      "Epoch 14/20.. Train loss: 0.012.. Validation loss: 1.771.. \n",
      "Epoch 14/20.. Train loss: 0.367.. Validation loss: 1.741.. \n",
      "Epoch 14/20.. Train loss: 0.808.. Validation loss: 1.697.. \n",
      "Epoch 14/20.. Train loss: 2.594.. Validation loss: 1.841.. \n",
      "Epoch 14/20.. Train loss: 0.685.. Validation loss: 1.922.. \n",
      "Epoch 14/20.. Train loss: 0.443.. Validation loss: 1.992.. \n",
      "Epoch 14/20.. Train loss: 0.010.. Validation loss: 2.002.. \n",
      "Epoch 14/20.. Train loss: 0.544.. Validation loss: 1.977.. \n",
      "Epoch 14/20.. Train loss: 1.817.. Validation loss: 1.859.. \n",
      "Epoch 14/20.. Train loss: 1.474.. Validation loss: 1.835.. \n",
      "Epoch 14/20.. Train loss: 0.030.. Validation loss: 1.828.. \n",
      "Epoch 14/20.. Train loss: 0.058.. Validation loss: 1.84.. \n",
      "Epoch 14/20.. Train loss: 0.049.. Validation loss: 1.837.. \n",
      "Epoch 14/20.. Train loss: 1.417.. Validation loss: 1.72.. \n",
      "Epoch 14/20.. Train loss: 0.591.. Validation loss: 1.705.. \n",
      "Epoch 14/20.. Train loss: 0.900.. Validation loss: 1.654.. \n",
      "Epoch 14/20.. Train loss: 0.084.. Validation loss: 1.647.. \n",
      "Epoch 14/20.. Train loss: 0.689.. Validation loss: 1.712.. \n",
      "Epoch 14/20.. Train loss: 0.006.. Validation loss: 1.705.. \n",
      "Epoch 14/20.. Train loss: 0.157.. Validation loss: 1.684.. \n",
      "Epoch 14/20.. Train loss: 0.424.. Validation loss: 1.731.. \n",
      "Epoch 14/20.. Train loss: 0.425.. Validation loss: 1.694.. \n",
      "Epoch 14/20.. Train loss: 2.126.. Validation loss: 1.651.. \n",
      "Epoch 14/20.. Train loss: 0.981.. Validation loss: 1.718.. \n",
      "Epoch 14/20.. Train loss: 3.817.. Validation loss: 1.804.. \n",
      "Epoch 14/20.. Train loss: 7.455.. Validation loss: 1.495.. \n",
      "Epoch 14/20.. Train loss: 2.056.. Validation loss: 1.562.. \n",
      "Epoch 14/20.. Train loss: 0.044.. Validation loss: 1.575.. \n",
      "Epoch 14/20.. Train loss: 1.513.. Validation loss: 1.642.. \n",
      "Epoch 14/20.. Train loss: 0.027.. Validation loss: 1.627.. \n",
      "Epoch 14/20.. Train loss: 0.528.. Validation loss: 1.587.. \n",
      "Epoch 14/20.. Train loss: 0.069.. Validation loss: 1.579.. \n",
      "Epoch 14/20.. Train loss: 0.369.. Validation loss: 1.597.. \n",
      "Epoch 14/20.. Train loss: 0.207.. Validation loss: 1.61.. \n",
      "Epoch 14/20.. Train loss: 0.042.. Validation loss: 1.617.. \n",
      "Epoch 14/20.. Train loss: 0.494.. Validation loss: 1.571.. \n",
      "Epoch 14/20.. Train loss: 0.216.. Validation loss: 1.584.. \n",
      "Epoch 14/20.. Train loss: 0.150.. Validation loss: 1.557.. \n",
      "Epoch 14/20.. Train loss: 2.646.. Validation loss: 1.586.. \n",
      "Epoch 14/20.. Train loss: 3.352.. Validation loss: 1.436.. \n",
      "Epoch 14/20.. Train loss: 0.059.. Validation loss: 1.445.. \n",
      "Epoch 14/20.. Train loss: 0.172.. Validation loss: 1.431.. \n",
      "Epoch 14/20.. Train loss: 0.259.. Validation loss: 1.453.. \n",
      "Epoch 14/20.. Train loss: 3.320.. Validation loss: 1.431.. \n",
      "Epoch 14/20.. Train loss: 0.170.. Validation loss: 1.444.. \n",
      "Epoch 14/20.. Train loss: 0.153.. Validation loss: 1.453.. \n",
      "Epoch 14/20.. Train loss: 0.007.. Validation loss: 1.455.. \n",
      "Epoch 14/20.. Train loss: 0.000.. Validation loss: 1.456.. \n",
      "Epoch 14/20.. Train loss: 0.002.. Validation loss: 1.454.. \n",
      "Epoch 14/20.. Train loss: 0.008.. Validation loss: 1.45.. \n",
      "Epoch 14/20.. Train loss: 0.071.. Validation loss: 1.456.. \n",
      "Epoch 14/20.. Train loss: 0.056.. Validation loss: 1.463.. \n",
      "Epoch 14/20.. Train loss: 0.379.. Validation loss: 1.44.. \n",
      "Epoch 14/20.. Train loss: 24.298.. Validation loss: 1.681.. \n",
      "Epoch 14/20.. Train loss: 0.809.. Validation loss: 1.598.. \n",
      "Epoch 14/20.. Train loss: 0.516.. Validation loss: 1.64.. \n",
      "Epoch 14/20.. Train loss: 0.630.. Validation loss: 1.592.. \n",
      "Epoch 14/20.. Train loss: 0.425.. Validation loss: 1.632.. \n",
      "Epoch 14/20.. Train loss: 1.744.. Validation loss: 1.736.. \n",
      "Epoch 14/20.. Train loss: 0.147.. Validation loss: 1.773.. \n",
      "Epoch 14/20.. Train loss: 0.150.. Validation loss: 1.75.. \n",
      "Epoch 14/20.. Train loss: 0.002.. Validation loss: 1.748.. \n",
      "Epoch 14/20.. Train loss: 0.661.. Validation loss: 1.692.. \n",
      "Epoch 14/20.. Train loss: 0.273.. Validation loss: 1.657.. \n",
      "Epoch 14/20.. Train loss: 7.378.. Validation loss: 1.435.. \n",
      "Epoch 14/20.. Train loss: 0.298.. Validation loss: 1.421.. \n",
      "Epoch 14/20.. Train loss: 1.510.. Validation loss: 1.45.. \n",
      "Epoch 14/20.. Train loss: 0.417.. Validation loss: 1.476.. \n",
      "Epoch 14/20.. Train loss: 0.468.. Validation loss: 1.509.. \n",
      "Epoch 14/20.. Train loss: 0.097.. Validation loss: 1.517.. \n",
      "Epoch 14/20.. Train loss: 2.879.. Validation loss: 1.623.. \n",
      "Epoch 14/20.. Train loss: 0.092.. Validation loss: 1.643.. \n",
      "Epoch 14/20.. Train loss: 1.174.. Validation loss: 1.712.. \n",
      "Epoch 14/20.. Train loss: 0.278.. Validation loss: 1.766.. \n",
      "Epoch 14/20.. Train loss: 0.719.. Validation loss: 1.83.. \n",
      "Epoch 14/20.. Train loss: 0.315.. Validation loss: 1.887.. \n",
      "Epoch 14/20.. Train loss: 0.004.. Validation loss: 1.894.. \n",
      "Epoch 14/20.. Train loss: 0.454.. Validation loss: 1.865.. \n",
      "Epoch 14/20.. Train loss: 1.677.. Validation loss: 1.99.. \n",
      "Epoch 14/20.. Train loss: 0.128.. Validation loss: 2.04.. \n",
      "Epoch 14/20.. Train loss: 2.188.. Validation loss: 1.914.. \n",
      "Epoch 14/20.. Train loss: 0.349.. Validation loss: 1.958.. \n",
      "Epoch 14/20.. Train loss: 0.148.. Validation loss: 2.011.. \n",
      "Epoch 14/20.. Train loss: 1.165.. Validation loss: 1.94.. \n",
      "Epoch 14/20.. Train loss: 0.682.. Validation loss: 1.883.. \n",
      "Epoch 14/20.. Train loss: 50.597.. Validation loss: 3.087.. \n",
      "Epoch 14/20.. Train loss: 2.343.. Validation loss: 2.82.. \n",
      "Epoch 14/20.. Train loss: 0.044.. Validation loss: 2.851.. \n",
      "Epoch 14/20.. Train loss: 2.912.. Validation loss: 2.529.. \n",
      "Epoch 14/20.. Train loss: 1.735.. Validation loss: 2.474.. \n",
      "Epoch 14/20.. Train loss: 0.673.. Validation loss: 2.575.. \n",
      "Epoch 14/20.. Train loss: 9.454.. Validation loss: 1.75.. \n",
      "Epoch 14/20.. Train loss: 0.983.. Validation loss: 1.842.. \n",
      "Epoch 14/20.. Train loss: 0.017.. Validation loss: 1.854.. \n",
      "Epoch 14/20.. Train loss: 0.442.. Validation loss: 1.767.. \n",
      "Epoch 15/20.. Train loss: 0.007.. Validation loss: 1.771.. \n",
      "Epoch 15/20.. Train loss: 0.428.. Validation loss: 1.738.. \n",
      "Epoch 15/20.. Train loss: 0.799.. Validation loss: 1.693.. \n",
      "Epoch 15/20.. Train loss: 2.567.. Validation loss: 1.837.. \n",
      "Epoch 15/20.. Train loss: 0.668.. Validation loss: 1.917.. \n",
      "Epoch 15/20.. Train loss: 0.432.. Validation loss: 1.987.. \n",
      "Epoch 15/20.. Train loss: 0.009.. Validation loss: 1.996.. \n",
      "Epoch 15/20.. Train loss: 0.574.. Validation loss: 1.97.. \n",
      "Epoch 15/20.. Train loss: 1.814.. Validation loss: 1.852.. \n",
      "Epoch 15/20.. Train loss: 1.478.. Validation loss: 1.828.. \n",
      "Epoch 15/20.. Train loss: 0.025.. Validation loss: 1.821.. \n",
      "Epoch 15/20.. Train loss: 0.063.. Validation loss: 1.834.. \n",
      "Epoch 15/20.. Train loss: 0.054.. Validation loss: 1.831.. \n",
      "Epoch 15/20.. Train loss: 1.398.. Validation loss: 1.715.. \n",
      "Epoch 15/20.. Train loss: 0.634.. Validation loss: 1.7.. \n",
      "Epoch 15/20.. Train loss: 0.912.. Validation loss: 1.648.. \n",
      "Epoch 15/20.. Train loss: 0.103.. Validation loss: 1.64.. \n",
      "Epoch 15/20.. Train loss: 0.680.. Validation loss: 1.704.. \n",
      "Epoch 15/20.. Train loss: 0.005.. Validation loss: 1.698.. \n",
      "Epoch 15/20.. Train loss: 0.158.. Validation loss: 1.677.. \n",
      "Epoch 15/20.. Train loss: 0.423.. Validation loss: 1.724.. \n",
      "Epoch 15/20.. Train loss: 0.431.. Validation loss: 1.687.. \n",
      "Epoch 15/20.. Train loss: 2.043.. Validation loss: 1.644.. \n",
      "Epoch 15/20.. Train loss: 0.963.. Validation loss: 1.711.. \n",
      "Epoch 15/20.. Train loss: 3.802.. Validation loss: 1.797.. \n",
      "Epoch 15/20.. Train loss: 7.343.. Validation loss: 1.491.. \n",
      "Epoch 15/20.. Train loss: 2.042.. Validation loss: 1.558.. \n",
      "Epoch 15/20.. Train loss: 0.043.. Validation loss: 1.57.. \n",
      "Epoch 15/20.. Train loss: 1.521.. Validation loss: 1.637.. \n",
      "Epoch 15/20.. Train loss: 0.027.. Validation loss: 1.622.. \n",
      "Epoch 15/20.. Train loss: 0.505.. Validation loss: 1.583.. \n",
      "Epoch 15/20.. Train loss: 0.068.. Validation loss: 1.576.. \n",
      "Epoch 15/20.. Train loss: 0.341.. Validation loss: 1.593.. \n",
      "Epoch 15/20.. Train loss: 0.189.. Validation loss: 1.606.. \n",
      "Epoch 15/20.. Train loss: 0.033.. Validation loss: 1.611.. \n",
      "Epoch 15/20.. Train loss: 0.490.. Validation loss: 1.566.. \n",
      "Epoch 15/20.. Train loss: 0.198.. Validation loss: 1.579.. \n",
      "Epoch 15/20.. Train loss: 0.148.. Validation loss: 1.552.. \n",
      "Epoch 15/20.. Train loss: 2.660.. Validation loss: 1.581.. \n",
      "Epoch 15/20.. Train loss: 3.354.. Validation loss: 1.431.. \n",
      "Epoch 15/20.. Train loss: 0.057.. Validation loss: 1.44.. \n",
      "Epoch 15/20.. Train loss: 0.171.. Validation loss: 1.426.. \n",
      "Epoch 15/20.. Train loss: 0.269.. Validation loss: 1.448.. \n",
      "Epoch 15/20.. Train loss: 3.199.. Validation loss: 1.427.. \n",
      "Epoch 15/20.. Train loss: 0.161.. Validation loss: 1.44.. \n",
      "Epoch 15/20.. Train loss: 0.164.. Validation loss: 1.448.. \n",
      "Epoch 15/20.. Train loss: 0.006.. Validation loss: 1.45.. \n",
      "Epoch 15/20.. Train loss: 0.000.. Validation loss: 1.451.. \n",
      "Epoch 15/20.. Train loss: 0.003.. Validation loss: 1.448.. \n",
      "Epoch 15/20.. Train loss: 0.008.. Validation loss: 1.445.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20.. Train loss: 0.051.. Validation loss: 1.45.. \n",
      "Epoch 15/20.. Train loss: 0.048.. Validation loss: 1.456.. \n",
      "Epoch 15/20.. Train loss: 0.373.. Validation loss: 1.433.. \n",
      "Epoch 15/20.. Train loss: 24.320.. Validation loss: 1.673.. \n",
      "Epoch 15/20.. Train loss: 0.803.. Validation loss: 1.591.. \n",
      "Epoch 15/20.. Train loss: 0.520.. Validation loss: 1.632.. \n",
      "Epoch 15/20.. Train loss: 0.624.. Validation loss: 1.585.. \n",
      "Epoch 15/20.. Train loss: 0.428.. Validation loss: 1.625.. \n",
      "Epoch 15/20.. Train loss: 1.732.. Validation loss: 1.728.. \n",
      "Epoch 15/20.. Train loss: 0.154.. Validation loss: 1.766.. \n",
      "Epoch 15/20.. Train loss: 0.150.. Validation loss: 1.743.. \n",
      "Epoch 15/20.. Train loss: 0.003.. Validation loss: 1.741.. \n",
      "Epoch 15/20.. Train loss: 0.657.. Validation loss: 1.686.. \n",
      "Epoch 15/20.. Train loss: 0.272.. Validation loss: 1.65.. \n",
      "Epoch 15/20.. Train loss: 7.300.. Validation loss: 1.431.. \n",
      "Epoch 15/20.. Train loss: 0.283.. Validation loss: 1.416.. \n",
      "Epoch 15/20.. Train loss: 1.488.. Validation loss: 1.445.. \n",
      "Epoch 15/20.. Train loss: 0.411.. Validation loss: 1.472.. \n",
      "Epoch 15/20.. Train loss: 0.464.. Validation loss: 1.504.. \n",
      "Epoch 15/20.. Train loss: 0.073.. Validation loss: 1.511.. \n",
      "Epoch 15/20.. Train loss: 2.876.. Validation loss: 1.617.. \n",
      "Epoch 15/20.. Train loss: 0.095.. Validation loss: 1.637.. \n",
      "Epoch 15/20.. Train loss: 1.189.. Validation loss: 1.706.. \n",
      "Epoch 15/20.. Train loss: 0.282.. Validation loss: 1.76.. \n",
      "Epoch 15/20.. Train loss: 0.733.. Validation loss: 1.824.. \n",
      "Epoch 15/20.. Train loss: 0.310.. Validation loss: 1.881.. \n",
      "Epoch 15/20.. Train loss: 0.004.. Validation loss: 1.887.. \n",
      "Epoch 15/20.. Train loss: 0.502.. Validation loss: 1.857.. \n",
      "Epoch 15/20.. Train loss: 1.669.. Validation loss: 1.981.. \n",
      "Epoch 15/20.. Train loss: 0.130.. Validation loss: 2.031.. \n",
      "Epoch 15/20.. Train loss: 2.211.. Validation loss: 1.905.. \n",
      "Epoch 15/20.. Train loss: 0.340.. Validation loss: 1.949.. \n",
      "Epoch 15/20.. Train loss: 0.151.. Validation loss: 2.001.. \n",
      "Epoch 15/20.. Train loss: 1.143.. Validation loss: 1.932.. \n",
      "Epoch 15/20.. Train loss: 0.663.. Validation loss: 1.875.. \n",
      "Epoch 15/20.. Train loss: 50.709.. Validation loss: 3.079.. \n",
      "Epoch 15/20.. Train loss: 2.368.. Validation loss: 2.81.. \n",
      "Epoch 15/20.. Train loss: 0.042.. Validation loss: 2.841.. \n",
      "Epoch 15/20.. Train loss: 2.878.. Validation loss: 2.521.. \n",
      "Epoch 15/20.. Train loss: 1.664.. Validation loss: 2.468.. \n",
      "Epoch 15/20.. Train loss: 0.677.. Validation loss: 2.569.. \n",
      "Epoch 15/20.. Train loss: 9.290.. Validation loss: 1.751.. \n",
      "Epoch 15/20.. Train loss: 0.968.. Validation loss: 1.842.. \n",
      "Epoch 15/20.. Train loss: 0.016.. Validation loss: 1.853.. \n",
      "Epoch 15/20.. Train loss: 0.437.. Validation loss: 1.767.. \n",
      "Epoch 16/20.. Train loss: 0.004.. Validation loss: 1.77.. \n",
      "Epoch 16/20.. Train loss: 0.482.. Validation loss: 1.735.. \n",
      "Epoch 16/20.. Train loss: 0.786.. Validation loss: 1.691.. \n",
      "Epoch 16/20.. Train loss: 2.542.. Validation loss: 1.834.. \n",
      "Epoch 16/20.. Train loss: 0.655.. Validation loss: 1.913.. \n",
      "Epoch 16/20.. Train loss: 0.423.. Validation loss: 1.982.. \n",
      "Epoch 16/20.. Train loss: 0.008.. Validation loss: 1.99.. \n",
      "Epoch 16/20.. Train loss: 0.597.. Validation loss: 1.964.. \n",
      "Epoch 16/20.. Train loss: 1.807.. Validation loss: 1.847.. \n",
      "Epoch 16/20.. Train loss: 1.473.. Validation loss: 1.823.. \n",
      "Epoch 16/20.. Train loss: 0.020.. Validation loss: 1.817.. \n",
      "Epoch 16/20.. Train loss: 0.069.. Validation loss: 1.83.. \n",
      "Epoch 16/20.. Train loss: 0.059.. Validation loss: 1.827.. \n",
      "Epoch 16/20.. Train loss: 1.383.. Validation loss: 1.711.. \n",
      "Epoch 16/20.. Train loss: 0.668.. Validation loss: 1.696.. \n",
      "Epoch 16/20.. Train loss: 0.921.. Validation loss: 1.644.. \n",
      "Epoch 16/20.. Train loss: 0.121.. Validation loss: 1.635.. \n",
      "Epoch 16/20.. Train loss: 0.671.. Validation loss: 1.699.. \n",
      "Epoch 16/20.. Train loss: 0.005.. Validation loss: 1.692.. \n",
      "Epoch 16/20.. Train loss: 0.158.. Validation loss: 1.672.. \n",
      "Epoch 16/20.. Train loss: 0.423.. Validation loss: 1.718.. \n",
      "Epoch 16/20.. Train loss: 0.436.. Validation loss: 1.681.. \n",
      "Epoch 16/20.. Train loss: 1.959.. Validation loss: 1.64.. \n",
      "Epoch 16/20.. Train loss: 0.946.. Validation loss: 1.705.. \n",
      "Epoch 16/20.. Train loss: 3.789.. Validation loss: 1.791.. \n",
      "Epoch 16/20.. Train loss: 7.259.. Validation loss: 1.487.. \n",
      "Epoch 16/20.. Train loss: 2.030.. Validation loss: 1.554.. \n",
      "Epoch 16/20.. Train loss: 0.042.. Validation loss: 1.566.. \n",
      "Epoch 16/20.. Train loss: 1.530.. Validation loss: 1.634.. \n",
      "Epoch 16/20.. Train loss: 0.026.. Validation loss: 1.619.. \n",
      "Epoch 16/20.. Train loss: 0.484.. Validation loss: 1.581.. \n",
      "Epoch 16/20.. Train loss: 0.065.. Validation loss: 1.573.. \n",
      "Epoch 16/20.. Train loss: 0.319.. Validation loss: 1.59.. \n",
      "Epoch 16/20.. Train loss: 0.175.. Validation loss: 1.602.. \n",
      "Epoch 16/20.. Train loss: 0.027.. Validation loss: 1.608.. \n",
      "Epoch 16/20.. Train loss: 0.487.. Validation loss: 1.562.. \n",
      "Epoch 16/20.. Train loss: 0.184.. Validation loss: 1.575.. \n",
      "Epoch 16/20.. Train loss: 0.146.. Validation loss: 1.548.. \n",
      "Epoch 16/20.. Train loss: 2.679.. Validation loss: 1.577.. \n",
      "Epoch 16/20.. Train loss: 3.361.. Validation loss: 1.428.. \n",
      "Epoch 16/20.. Train loss: 0.056.. Validation loss: 1.436.. \n",
      "Epoch 16/20.. Train loss: 0.170.. Validation loss: 1.422.. \n",
      "Epoch 16/20.. Train loss: 0.276.. Validation loss: 1.444.. \n",
      "Epoch 16/20.. Train loss: 3.075.. Validation loss: 1.424.. \n",
      "Epoch 16/20.. Train loss: 0.153.. Validation loss: 1.436.. \n",
      "Epoch 16/20.. Train loss: 0.176.. Validation loss: 1.445.. \n",
      "Epoch 16/20.. Train loss: 0.004.. Validation loss: 1.447.. \n",
      "Epoch 16/20.. Train loss: 0.000.. Validation loss: 1.447.. \n",
      "Epoch 16/20.. Train loss: 0.004.. Validation loss: 1.444.. \n",
      "Epoch 16/20.. Train loss: 0.008.. Validation loss: 1.441.. \n",
      "Epoch 16/20.. Train loss: 0.037.. Validation loss: 1.445.. \n",
      "Epoch 16/20.. Train loss: 0.042.. Validation loss: 1.451.. \n",
      "Epoch 16/20.. Train loss: 0.367.. Validation loss: 1.428.. \n",
      "Epoch 16/20.. Train loss: 24.343.. Validation loss: 1.666.. \n",
      "Epoch 16/20.. Train loss: 0.799.. Validation loss: 1.585.. \n",
      "Epoch 16/20.. Train loss: 0.525.. Validation loss: 1.626.. \n",
      "Epoch 16/20.. Train loss: 0.618.. Validation loss: 1.579.. \n",
      "Epoch 16/20.. Train loss: 0.431.. Validation loss: 1.62.. \n",
      "Epoch 16/20.. Train loss: 1.719.. Validation loss: 1.721.. \n",
      "Epoch 16/20.. Train loss: 0.160.. Validation loss: 1.76.. \n",
      "Epoch 16/20.. Train loss: 0.149.. Validation loss: 1.737.. \n",
      "Epoch 16/20.. Train loss: 0.003.. Validation loss: 1.735.. \n",
      "Epoch 16/20.. Train loss: 0.653.. Validation loss: 1.68.. \n",
      "Epoch 16/20.. Train loss: 0.272.. Validation loss: 1.645.. \n",
      "Epoch 16/20.. Train loss: 7.237.. Validation loss: 1.427.. \n",
      "Epoch 16/20.. Train loss: 0.269.. Validation loss: 1.413.. \n",
      "Epoch 16/20.. Train loss: 1.467.. Validation loss: 1.442.. \n",
      "Epoch 16/20.. Train loss: 0.405.. Validation loss: 1.468.. \n",
      "Epoch 16/20.. Train loss: 0.459.. Validation loss: 1.5.. \n",
      "Epoch 16/20.. Train loss: 0.054.. Validation loss: 1.506.. \n",
      "Epoch 16/20.. Train loss: 2.873.. Validation loss: 1.612.. \n",
      "Epoch 16/20.. Train loss: 0.098.. Validation loss: 1.632.. \n",
      "Epoch 16/20.. Train loss: 1.203.. Validation loss: 1.701.. \n",
      "Epoch 16/20.. Train loss: 0.284.. Validation loss: 1.755.. \n",
      "Epoch 16/20.. Train loss: 0.748.. Validation loss: 1.82.. \n",
      "Epoch 16/20.. Train loss: 0.305.. Validation loss: 1.876.. \n",
      "Epoch 16/20.. Train loss: 0.003.. Validation loss: 1.882.. \n",
      "Epoch 16/20.. Train loss: 0.544.. Validation loss: 1.851.. \n",
      "Epoch 16/20.. Train loss: 1.660.. Validation loss: 1.974.. \n",
      "Epoch 16/20.. Train loss: 0.132.. Validation loss: 2.024.. \n",
      "Epoch 16/20.. Train loss: 2.229.. Validation loss: 1.898.. \n",
      "Epoch 16/20.. Train loss: 0.331.. Validation loss: 1.941.. \n",
      "Epoch 16/20.. Train loss: 0.153.. Validation loss: 1.994.. \n",
      "Epoch 16/20.. Train loss: 1.118.. Validation loss: 1.925.. \n",
      "Epoch 16/20.. Train loss: 0.644.. Validation loss: 1.869.. \n",
      "Epoch 16/20.. Train loss: 50.798.. Validation loss: 3.072.. \n",
      "Epoch 16/20.. Train loss: 2.391.. Validation loss: 2.802.. \n",
      "Epoch 16/20.. Train loss: 0.041.. Validation loss: 2.832.. \n",
      "Epoch 16/20.. Train loss: 2.852.. Validation loss: 2.514.. \n",
      "Epoch 16/20.. Train loss: 1.592.. Validation loss: 2.462.. \n",
      "Epoch 16/20.. Train loss: 0.680.. Validation loss: 2.563.. \n",
      "Epoch 16/20.. Train loss: 9.164.. Validation loss: 1.751.. \n",
      "Epoch 16/20.. Train loss: 0.955.. Validation loss: 1.842.. \n",
      "Epoch 16/20.. Train loss: 0.016.. Validation loss: 1.853.. \n",
      "Epoch 16/20.. Train loss: 0.434.. Validation loss: 1.767.. \n",
      "Epoch 17/20.. Train loss: 0.002.. Validation loss: 1.769.. \n",
      "Epoch 17/20.. Train loss: 0.532.. Validation loss: 1.732.. \n",
      "Epoch 17/20.. Train loss: 0.769.. Validation loss: 1.688.. \n",
      "Epoch 17/20.. Train loss: 2.519.. Validation loss: 1.831.. \n",
      "Epoch 17/20.. Train loss: 0.644.. Validation loss: 1.909.. \n",
      "Epoch 17/20.. Train loss: 0.416.. Validation loss: 1.977.. \n",
      "Epoch 17/20.. Train loss: 0.007.. Validation loss: 1.986.. \n",
      "Epoch 17/20.. Train loss: 0.614.. Validation loss: 1.959.. \n",
      "Epoch 17/20.. Train loss: 1.797.. Validation loss: 1.842.. \n",
      "Epoch 17/20.. Train loss: 1.462.. Validation loss: 1.818.. \n",
      "Epoch 17/20.. Train loss: 0.015.. Validation loss: 1.813.. \n",
      "Epoch 17/20.. Train loss: 0.076.. Validation loss: 1.827.. \n",
      "Epoch 17/20.. Train loss: 0.062.. Validation loss: 1.824.. \n",
      "Epoch 17/20.. Train loss: 1.370.. Validation loss: 1.709.. \n",
      "Epoch 17/20.. Train loss: 0.695.. Validation loss: 1.693.. \n",
      "Epoch 17/20.. Train loss: 0.926.. Validation loss: 1.641.. \n",
      "Epoch 17/20.. Train loss: 0.137.. Validation loss: 1.631.. \n",
      "Epoch 17/20.. Train loss: 0.662.. Validation loss: 1.694.. \n",
      "Epoch 17/20.. Train loss: 0.004.. Validation loss: 1.688.. \n",
      "Epoch 17/20.. Train loss: 0.158.. Validation loss: 1.668.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20.. Train loss: 0.423.. Validation loss: 1.714.. \n",
      "Epoch 17/20.. Train loss: 0.438.. Validation loss: 1.677.. \n",
      "Epoch 17/20.. Train loss: 1.876.. Validation loss: 1.637.. \n",
      "Epoch 17/20.. Train loss: 0.929.. Validation loss: 1.701.. \n",
      "Epoch 17/20.. Train loss: 3.778.. Validation loss: 1.787.. \n",
      "Epoch 17/20.. Train loss: 7.195.. Validation loss: 1.485.. \n",
      "Epoch 17/20.. Train loss: 2.018.. Validation loss: 1.552.. \n",
      "Epoch 17/20.. Train loss: 0.041.. Validation loss: 1.564.. \n",
      "Epoch 17/20.. Train loss: 1.539.. Validation loss: 1.631.. \n",
      "Epoch 17/20.. Train loss: 0.027.. Validation loss: 1.617.. \n",
      "Epoch 17/20.. Train loss: 0.464.. Validation loss: 1.579.. \n",
      "Epoch 17/20.. Train loss: 0.062.. Validation loss: 1.571.. \n",
      "Epoch 17/20.. Train loss: 0.300.. Validation loss: 1.588.. \n",
      "Epoch 17/20.. Train loss: 0.163.. Validation loss: 1.6.. \n",
      "Epoch 17/20.. Train loss: 0.023.. Validation loss: 1.605.. \n",
      "Epoch 17/20.. Train loss: 0.484.. Validation loss: 1.56.. \n",
      "Epoch 17/20.. Train loss: 0.172.. Validation loss: 1.571.. \n",
      "Epoch 17/20.. Train loss: 0.146.. Validation loss: 1.545.. \n",
      "Epoch 17/20.. Train loss: 2.701.. Validation loss: 1.574.. \n",
      "Epoch 17/20.. Train loss: 3.371.. Validation loss: 1.425.. \n",
      "Epoch 17/20.. Train loss: 0.054.. Validation loss: 1.433.. \n",
      "Epoch 17/20.. Train loss: 0.170.. Validation loss: 1.419.. \n",
      "Epoch 17/20.. Train loss: 0.281.. Validation loss: 1.441.. \n",
      "Epoch 17/20.. Train loss: 2.951.. Validation loss: 1.421.. \n",
      "Epoch 17/20.. Train loss: 0.145.. Validation loss: 1.433.. \n",
      "Epoch 17/20.. Train loss: 0.188.. Validation loss: 1.443.. \n",
      "Epoch 17/20.. Train loss: 0.003.. Validation loss: 1.444.. \n",
      "Epoch 17/20.. Train loss: 0.000.. Validation loss: 1.444.. \n",
      "Epoch 17/20.. Train loss: 0.005.. Validation loss: 1.441.. \n",
      "Epoch 17/20.. Train loss: 0.007.. Validation loss: 1.437.. \n",
      "Epoch 17/20.. Train loss: 0.026.. Validation loss: 1.441.. \n",
      "Epoch 17/20.. Train loss: 0.037.. Validation loss: 1.446.. \n",
      "Epoch 17/20.. Train loss: 0.363.. Validation loss: 1.424.. \n",
      "Epoch 17/20.. Train loss: 24.366.. Validation loss: 1.66.. \n",
      "Epoch 17/20.. Train loss: 0.798.. Validation loss: 1.58.. \n",
      "Epoch 17/20.. Train loss: 0.530.. Validation loss: 1.621.. \n",
      "Epoch 17/20.. Train loss: 0.611.. Validation loss: 1.575.. \n",
      "Epoch 17/20.. Train loss: 0.435.. Validation loss: 1.615.. \n",
      "Epoch 17/20.. Train loss: 1.707.. Validation loss: 1.716.. \n",
      "Epoch 17/20.. Train loss: 0.166.. Validation loss: 1.755.. \n",
      "Epoch 17/20.. Train loss: 0.146.. Validation loss: 1.732.. \n",
      "Epoch 17/20.. Train loss: 0.003.. Validation loss: 1.73.. \n",
      "Epoch 17/20.. Train loss: 0.648.. Validation loss: 1.676.. \n",
      "Epoch 17/20.. Train loss: 0.272.. Validation loss: 1.641.. \n",
      "Epoch 17/20.. Train loss: 7.184.. Validation loss: 1.425.. \n",
      "Epoch 17/20.. Train loss: 0.256.. Validation loss: 1.411.. \n",
      "Epoch 17/20.. Train loss: 1.448.. Validation loss: 1.44.. \n",
      "Epoch 17/20.. Train loss: 0.400.. Validation loss: 1.465.. \n",
      "Epoch 17/20.. Train loss: 0.454.. Validation loss: 1.498.. \n",
      "Epoch 17/20.. Train loss: 0.040.. Validation loss: 1.502.. \n",
      "Epoch 17/20.. Train loss: 2.869.. Validation loss: 1.607.. \n",
      "Epoch 17/20.. Train loss: 0.101.. Validation loss: 1.628.. \n",
      "Epoch 17/20.. Train loss: 1.217.. Validation loss: 1.697.. \n",
      "Epoch 17/20.. Train loss: 0.285.. Validation loss: 1.751.. \n",
      "Epoch 17/20.. Train loss: 0.762.. Validation loss: 1.816.. \n",
      "Epoch 17/20.. Train loss: 0.300.. Validation loss: 1.872.. \n",
      "Epoch 17/20.. Train loss: 0.003.. Validation loss: 1.878.. \n",
      "Epoch 17/20.. Train loss: 0.581.. Validation loss: 1.845.. \n",
      "Epoch 17/20.. Train loss: 1.651.. Validation loss: 1.968.. \n",
      "Epoch 17/20.. Train loss: 0.133.. Validation loss: 2.018.. \n",
      "Epoch 17/20.. Train loss: 2.245.. Validation loss: 1.892.. \n",
      "Epoch 17/20.. Train loss: 0.324.. Validation loss: 1.934.. \n",
      "Epoch 17/20.. Train loss: 0.154.. Validation loss: 1.987.. \n",
      "Epoch 17/20.. Train loss: 1.092.. Validation loss: 1.92.. \n",
      "Epoch 17/20.. Train loss: 0.624.. Validation loss: 1.865.. \n",
      "Epoch 17/20.. Train loss: 50.868.. Validation loss: 3.066.. \n",
      "Epoch 17/20.. Train loss: 2.412.. Validation loss: 2.795.. \n",
      "Epoch 17/20.. Train loss: 0.039.. Validation loss: 2.825.. \n",
      "Epoch 17/20.. Train loss: 2.832.. Validation loss: 2.509.. \n",
      "Epoch 17/20.. Train loss: 1.522.. Validation loss: 2.458.. \n",
      "Epoch 17/20.. Train loss: 0.684.. Validation loss: 2.559.. \n",
      "Epoch 17/20.. Train loss: 9.064.. Validation loss: 1.751.. \n",
      "Epoch 17/20.. Train loss: 0.944.. Validation loss: 1.841.. \n",
      "Epoch 17/20.. Train loss: 0.016.. Validation loss: 1.852.. \n",
      "Epoch 17/20.. Train loss: 0.432.. Validation loss: 1.766.. \n",
      "Epoch 18/20.. Train loss: 0.001.. Validation loss: 1.768.. \n",
      "Epoch 18/20.. Train loss: 0.577.. Validation loss: 1.729.. \n",
      "Epoch 18/20.. Train loss: 0.751.. Validation loss: 1.686.. \n",
      "Epoch 18/20.. Train loss: 2.497.. Validation loss: 1.828.. \n",
      "Epoch 18/20.. Train loss: 0.635.. Validation loss: 1.906.. \n",
      "Epoch 18/20.. Train loss: 0.411.. Validation loss: 1.973.. \n",
      "Epoch 18/20.. Train loss: 0.007.. Validation loss: 1.981.. \n",
      "Epoch 18/20.. Train loss: 0.627.. Validation loss: 1.955.. \n",
      "Epoch 18/20.. Train loss: 1.785.. Validation loss: 1.838.. \n",
      "Epoch 18/20.. Train loss: 1.447.. Validation loss: 1.815.. \n",
      "Epoch 18/20.. Train loss: 0.011.. Validation loss: 1.81.. \n",
      "Epoch 18/20.. Train loss: 0.083.. Validation loss: 1.824.. \n",
      "Epoch 18/20.. Train loss: 0.065.. Validation loss: 1.821.. \n",
      "Epoch 18/20.. Train loss: 1.358.. Validation loss: 1.707.. \n",
      "Epoch 18/20.. Train loss: 0.718.. Validation loss: 1.69.. \n",
      "Epoch 18/20.. Train loss: 0.929.. Validation loss: 1.639.. \n",
      "Epoch 18/20.. Train loss: 0.152.. Validation loss: 1.628.. \n",
      "Epoch 18/20.. Train loss: 0.653.. Validation loss: 1.691.. \n",
      "Epoch 18/20.. Train loss: 0.004.. Validation loss: 1.685.. \n",
      "Epoch 18/20.. Train loss: 0.156.. Validation loss: 1.665.. \n",
      "Epoch 18/20.. Train loss: 0.423.. Validation loss: 1.711.. \n",
      "Epoch 18/20.. Train loss: 0.439.. Validation loss: 1.674.. \n",
      "Epoch 18/20.. Train loss: 1.795.. Validation loss: 1.634.. \n",
      "Epoch 18/20.. Train loss: 0.913.. Validation loss: 1.698.. \n",
      "Epoch 18/20.. Train loss: 3.768.. Validation loss: 1.784.. \n",
      "Epoch 18/20.. Train loss: 7.145.. Validation loss: 1.483.. \n",
      "Epoch 18/20.. Train loss: 2.007.. Validation loss: 1.55.. \n",
      "Epoch 18/20.. Train loss: 0.040.. Validation loss: 1.562.. \n",
      "Epoch 18/20.. Train loss: 1.548.. Validation loss: 1.629.. \n",
      "Epoch 18/20.. Train loss: 0.027.. Validation loss: 1.615.. \n",
      "Epoch 18/20.. Train loss: 0.446.. Validation loss: 1.578.. \n",
      "Epoch 18/20.. Train loss: 0.058.. Validation loss: 1.57.. \n",
      "Epoch 18/20.. Train loss: 0.284.. Validation loss: 1.586.. \n",
      "Epoch 18/20.. Train loss: 0.153.. Validation loss: 1.598.. \n",
      "Epoch 18/20.. Train loss: 0.019.. Validation loss: 1.602.. \n",
      "Epoch 18/20.. Train loss: 0.483.. Validation loss: 1.557.. \n",
      "Epoch 18/20.. Train loss: 0.163.. Validation loss: 1.569.. \n",
      "Epoch 18/20.. Train loss: 0.146.. Validation loss: 1.542.. \n",
      "Epoch 18/20.. Train loss: 2.725.. Validation loss: 1.572.. \n",
      "Epoch 18/20.. Train loss: 3.383.. Validation loss: 1.423.. \n",
      "Epoch 18/20.. Train loss: 0.053.. Validation loss: 1.431.. \n",
      "Epoch 18/20.. Train loss: 0.170.. Validation loss: 1.417.. \n",
      "Epoch 18/20.. Train loss: 0.285.. Validation loss: 1.439.. \n",
      "Epoch 18/20.. Train loss: 2.830.. Validation loss: 1.42.. \n",
      "Epoch 18/20.. Train loss: 0.138.. Validation loss: 1.431.. \n",
      "Epoch 18/20.. Train loss: 0.201.. Validation loss: 1.441.. \n",
      "Epoch 18/20.. Train loss: 0.003.. Validation loss: 1.442.. \n",
      "Epoch 18/20.. Train loss: 0.000.. Validation loss: 1.442.. \n",
      "Epoch 18/20.. Train loss: 0.006.. Validation loss: 1.438.. \n",
      "Epoch 18/20.. Train loss: 0.007.. Validation loss: 1.435.. \n",
      "Epoch 18/20.. Train loss: 0.018.. Validation loss: 1.438.. \n",
      "Epoch 18/20.. Train loss: 0.034.. Validation loss: 1.443.. \n",
      "Epoch 18/20.. Train loss: 0.359.. Validation loss: 1.421.. \n",
      "Epoch 18/20.. Train loss: 24.388.. Validation loss: 1.655.. \n",
      "Epoch 18/20.. Train loss: 0.797.. Validation loss: 1.576.. \n",
      "Epoch 18/20.. Train loss: 0.536.. Validation loss: 1.617.. \n",
      "Epoch 18/20.. Train loss: 0.605.. Validation loss: 1.571.. \n",
      "Epoch 18/20.. Train loss: 0.438.. Validation loss: 1.611.. \n",
      "Epoch 18/20.. Train loss: 1.695.. Validation loss: 1.711.. \n",
      "Epoch 18/20.. Train loss: 0.170.. Validation loss: 1.751.. \n",
      "Epoch 18/20.. Train loss: 0.143.. Validation loss: 1.729.. \n",
      "Epoch 18/20.. Train loss: 0.003.. Validation loss: 1.727.. \n",
      "Epoch 18/20.. Train loss: 0.643.. Validation loss: 1.672.. \n",
      "Epoch 18/20.. Train loss: 0.271.. Validation loss: 1.637.. \n",
      "Epoch 18/20.. Train loss: 7.139.. Validation loss: 1.423.. \n",
      "Epoch 18/20.. Train loss: 0.243.. Validation loss: 1.409.. \n",
      "Epoch 18/20.. Train loss: 1.431.. Validation loss: 1.438.. \n",
      "Epoch 18/20.. Train loss: 0.394.. Validation loss: 1.463.. \n",
      "Epoch 18/20.. Train loss: 0.448.. Validation loss: 1.495.. \n",
      "Epoch 18/20.. Train loss: 0.029.. Validation loss: 1.499.. \n",
      "Epoch 18/20.. Train loss: 2.864.. Validation loss: 1.604.. \n",
      "Epoch 18/20.. Train loss: 0.104.. Validation loss: 1.624.. \n",
      "Epoch 18/20.. Train loss: 1.230.. Validation loss: 1.694.. \n",
      "Epoch 18/20.. Train loss: 0.286.. Validation loss: 1.748.. \n",
      "Epoch 18/20.. Train loss: 0.776.. Validation loss: 1.814.. \n",
      "Epoch 18/20.. Train loss: 0.296.. Validation loss: 1.869.. \n",
      "Epoch 18/20.. Train loss: 0.002.. Validation loss: 1.874.. \n",
      "Epoch 18/20.. Train loss: 0.616.. Validation loss: 1.841.. \n",
      "Epoch 18/20.. Train loss: 1.642.. Validation loss: 1.962.. \n",
      "Epoch 18/20.. Train loss: 0.133.. Validation loss: 2.012.. \n",
      "Epoch 18/20.. Train loss: 2.259.. Validation loss: 1.886.. \n",
      "Epoch 18/20.. Train loss: 0.317.. Validation loss: 1.928.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20.. Train loss: 0.155.. Validation loss: 1.981.. \n",
      "Epoch 18/20.. Train loss: 1.066.. Validation loss: 1.915.. \n",
      "Epoch 18/20.. Train loss: 0.605.. Validation loss: 1.861.. \n",
      "Epoch 18/20.. Train loss: 50.925.. Validation loss: 3.061.. \n",
      "Epoch 18/20.. Train loss: 2.433.. Validation loss: 2.789.. \n",
      "Epoch 18/20.. Train loss: 0.038.. Validation loss: 2.818.. \n",
      "Epoch 18/20.. Train loss: 2.816.. Validation loss: 2.504.. \n",
      "Epoch 18/20.. Train loss: 1.454.. Validation loss: 2.454.. \n",
      "Epoch 18/20.. Train loss: 0.688.. Validation loss: 2.555.. \n",
      "Epoch 18/20.. Train loss: 8.984.. Validation loss: 1.751.. \n",
      "Epoch 18/20.. Train loss: 0.934.. Validation loss: 1.841.. \n",
      "Epoch 18/20.. Train loss: 0.015.. Validation loss: 1.852.. \n",
      "Epoch 18/20.. Train loss: 0.432.. Validation loss: 1.766.. \n",
      "Epoch 19/20.. Train loss: 0.000.. Validation loss: 1.767.. \n",
      "Epoch 19/20.. Train loss: 0.619.. Validation loss: 1.727.. \n",
      "Epoch 19/20.. Train loss: 0.732.. Validation loss: 1.684.. \n",
      "Epoch 19/20.. Train loss: 2.476.. Validation loss: 1.825.. \n",
      "Epoch 19/20.. Train loss: 0.626.. Validation loss: 1.903.. \n",
      "Epoch 19/20.. Train loss: 0.406.. Validation loss: 1.97.. \n",
      "Epoch 19/20.. Train loss: 0.006.. Validation loss: 1.978.. \n",
      "Epoch 19/20.. Train loss: 0.636.. Validation loss: 1.951.. \n",
      "Epoch 19/20.. Train loss: 1.772.. Validation loss: 1.835.. \n",
      "Epoch 19/20.. Train loss: 1.430.. Validation loss: 1.811.. \n",
      "Epoch 19/20.. Train loss: 0.007.. Validation loss: 1.808.. \n",
      "Epoch 19/20.. Train loss: 0.090.. Validation loss: 1.823.. \n",
      "Epoch 19/20.. Train loss: 0.068.. Validation loss: 1.82.. \n",
      "Epoch 19/20.. Train loss: 1.349.. Validation loss: 1.705.. \n",
      "Epoch 19/20.. Train loss: 0.737.. Validation loss: 1.689.. \n",
      "Epoch 19/20.. Train loss: 0.931.. Validation loss: 1.637.. \n",
      "Epoch 19/20.. Train loss: 0.165.. Validation loss: 1.626.. \n",
      "Epoch 19/20.. Train loss: 0.645.. Validation loss: 1.688.. \n",
      "Epoch 19/20.. Train loss: 0.004.. Validation loss: 1.682.. \n",
      "Epoch 19/20.. Train loss: 0.155.. Validation loss: 1.662.. \n",
      "Epoch 19/20.. Train loss: 0.424.. Validation loss: 1.708.. \n",
      "Epoch 19/20.. Train loss: 0.440.. Validation loss: 1.671.. \n",
      "Epoch 19/20.. Train loss: 1.718.. Validation loss: 1.633.. \n",
      "Epoch 19/20.. Train loss: 0.897.. Validation loss: 1.696.. \n",
      "Epoch 19/20.. Train loss: 3.760.. Validation loss: 1.782.. \n",
      "Epoch 19/20.. Train loss: 7.105.. Validation loss: 1.482.. \n",
      "Epoch 19/20.. Train loss: 1.997.. Validation loss: 1.548.. \n",
      "Epoch 19/20.. Train loss: 0.039.. Validation loss: 1.56.. \n",
      "Epoch 19/20.. Train loss: 1.556.. Validation loss: 1.628.. \n",
      "Epoch 19/20.. Train loss: 0.028.. Validation loss: 1.613.. \n",
      "Epoch 19/20.. Train loss: 0.429.. Validation loss: 1.577.. \n",
      "Epoch 19/20.. Train loss: 0.054.. Validation loss: 1.57.. \n",
      "Epoch 19/20.. Train loss: 0.270.. Validation loss: 1.585.. \n",
      "Epoch 19/20.. Train loss: 0.145.. Validation loss: 1.597.. \n",
      "Epoch 19/20.. Train loss: 0.016.. Validation loss: 1.601.. \n",
      "Epoch 19/20.. Train loss: 0.482.. Validation loss: 1.556.. \n",
      "Epoch 19/20.. Train loss: 0.155.. Validation loss: 1.567.. \n",
      "Epoch 19/20.. Train loss: 0.146.. Validation loss: 1.54.. \n",
      "Epoch 19/20.. Train loss: 2.751.. Validation loss: 1.57.. \n",
      "Epoch 19/20.. Train loss: 3.396.. Validation loss: 1.421.. \n",
      "Epoch 19/20.. Train loss: 0.051.. Validation loss: 1.429.. \n",
      "Epoch 19/20.. Train loss: 0.171.. Validation loss: 1.415.. \n",
      "Epoch 19/20.. Train loss: 0.288.. Validation loss: 1.437.. \n",
      "Epoch 19/20.. Train loss: 2.713.. Validation loss: 1.418.. \n",
      "Epoch 19/20.. Train loss: 0.132.. Validation loss: 1.43.. \n",
      "Epoch 19/20.. Train loss: 0.213.. Validation loss: 1.439.. \n",
      "Epoch 19/20.. Train loss: 0.002.. Validation loss: 1.441.. \n",
      "Epoch 19/20.. Train loss: 0.001.. Validation loss: 1.44.. \n",
      "Epoch 19/20.. Train loss: 0.007.. Validation loss: 1.436.. \n",
      "Epoch 19/20.. Train loss: 0.008.. Validation loss: 1.433.. \n",
      "Epoch 19/20.. Train loss: 0.011.. Validation loss: 1.435.. \n",
      "Epoch 19/20.. Train loss: 0.031.. Validation loss: 1.44.. \n",
      "Epoch 19/20.. Train loss: 0.355.. Validation loss: 1.419.. \n",
      "Epoch 19/20.. Train loss: 24.410.. Validation loss: 1.651.. \n",
      "Epoch 19/20.. Train loss: 0.797.. Validation loss: 1.572.. \n",
      "Epoch 19/20.. Train loss: 0.542.. Validation loss: 1.613.. \n",
      "Epoch 19/20.. Train loss: 0.599.. Validation loss: 1.568.. \n",
      "Epoch 19/20.. Train loss: 0.442.. Validation loss: 1.608.. \n",
      "Epoch 19/20.. Train loss: 1.683.. Validation loss: 1.707.. \n",
      "Epoch 19/20.. Train loss: 0.175.. Validation loss: 1.747.. \n",
      "Epoch 19/20.. Train loss: 0.140.. Validation loss: 1.725.. \n",
      "Epoch 19/20.. Train loss: 0.002.. Validation loss: 1.723.. \n",
      "Epoch 19/20.. Train loss: 0.637.. Validation loss: 1.67.. \n",
      "Epoch 19/20.. Train loss: 0.271.. Validation loss: 1.635.. \n",
      "Epoch 19/20.. Train loss: 7.099.. Validation loss: 1.421.. \n",
      "Epoch 19/20.. Train loss: 0.232.. Validation loss: 1.408.. \n",
      "Epoch 19/20.. Train loss: 1.415.. Validation loss: 1.437.. \n",
      "Epoch 19/20.. Train loss: 0.389.. Validation loss: 1.462.. \n",
      "Epoch 19/20.. Train loss: 0.442.. Validation loss: 1.493.. \n",
      "Epoch 19/20.. Train loss: 0.021.. Validation loss: 1.497.. \n",
      "Epoch 19/20.. Train loss: 2.859.. Validation loss: 1.601.. \n",
      "Epoch 19/20.. Train loss: 0.106.. Validation loss: 1.621.. \n",
      "Epoch 19/20.. Train loss: 1.243.. Validation loss: 1.691.. \n",
      "Epoch 19/20.. Train loss: 0.286.. Validation loss: 1.745.. \n",
      "Epoch 19/20.. Train loss: 0.789.. Validation loss: 1.812.. \n",
      "Epoch 19/20.. Train loss: 0.292.. Validation loss: 1.866.. \n",
      "Epoch 19/20.. Train loss: 0.002.. Validation loss: 1.871.. \n",
      "Epoch 19/20.. Train loss: 0.647.. Validation loss: 1.837.. \n",
      "Epoch 19/20.. Train loss: 1.633.. Validation loss: 1.958.. \n",
      "Epoch 19/20.. Train loss: 0.133.. Validation loss: 2.008.. \n",
      "Epoch 19/20.. Train loss: 2.271.. Validation loss: 1.882.. \n",
      "Epoch 19/20.. Train loss: 0.311.. Validation loss: 1.923.. \n",
      "Epoch 19/20.. Train loss: 0.156.. Validation loss: 1.975.. \n",
      "Epoch 19/20.. Train loss: 1.039.. Validation loss: 1.91.. \n",
      "Epoch 19/20.. Train loss: 0.586.. Validation loss: 1.857.. \n",
      "Epoch 19/20.. Train loss: 50.971.. Validation loss: 3.056.. \n",
      "Epoch 19/20.. Train loss: 2.452.. Validation loss: 2.784.. \n",
      "Epoch 19/20.. Train loss: 0.037.. Validation loss: 2.813.. \n",
      "Epoch 19/20.. Train loss: 2.804.. Validation loss: 2.499.. \n",
      "Epoch 19/20.. Train loss: 1.389.. Validation loss: 2.45.. \n",
      "Epoch 19/20.. Train loss: 0.691.. Validation loss: 2.552.. \n",
      "Epoch 19/20.. Train loss: 8.919.. Validation loss: 1.751.. \n",
      "Epoch 19/20.. Train loss: 0.926.. Validation loss: 1.841.. \n",
      "Epoch 19/20.. Train loss: 0.015.. Validation loss: 1.852.. \n",
      "Epoch 19/20.. Train loss: 0.432.. Validation loss: 1.765.. \n",
      "Epoch 20/20.. Train loss: 0.000.. Validation loss: 1.766.. \n",
      "Epoch 20/20.. Train loss: 0.658.. Validation loss: 1.725.. \n",
      "Epoch 20/20.. Train loss: 0.713.. Validation loss: 1.683.. \n",
      "Epoch 20/20.. Train loss: 2.456.. Validation loss: 1.823.. \n",
      "Epoch 20/20.. Train loss: 0.619.. Validation loss: 1.9.. \n",
      "Epoch 20/20.. Train loss: 0.402.. Validation loss: 1.966.. \n",
      "Epoch 20/20.. Train loss: 0.006.. Validation loss: 1.974.. \n",
      "Epoch 20/20.. Train loss: 0.643.. Validation loss: 1.947.. \n",
      "Epoch 20/20.. Train loss: 1.758.. Validation loss: 1.832.. \n",
      "Epoch 20/20.. Train loss: 1.411.. Validation loss: 1.809.. \n",
      "Epoch 20/20.. Train loss: 0.004.. Validation loss: 1.806.. \n",
      "Epoch 20/20.. Train loss: 0.097.. Validation loss: 1.821.. \n",
      "Epoch 20/20.. Train loss: 0.070.. Validation loss: 1.818.. \n",
      "Epoch 20/20.. Train loss: 1.340.. Validation loss: 1.704.. \n",
      "Epoch 20/20.. Train loss: 0.753.. Validation loss: 1.687.. \n",
      "Epoch 20/20.. Train loss: 0.932.. Validation loss: 1.635.. \n",
      "Epoch 20/20.. Train loss: 0.177.. Validation loss: 1.624.. \n",
      "Epoch 20/20.. Train loss: 0.637.. Validation loss: 1.686.. \n",
      "Epoch 20/20.. Train loss: 0.004.. Validation loss: 1.68.. \n",
      "Epoch 20/20.. Train loss: 0.153.. Validation loss: 1.66.. \n",
      "Epoch 20/20.. Train loss: 0.424.. Validation loss: 1.706.. \n",
      "Epoch 20/20.. Train loss: 0.439.. Validation loss: 1.669.. \n",
      "Epoch 20/20.. Train loss: 1.644.. Validation loss: 1.632.. \n",
      "Epoch 20/20.. Train loss: 0.882.. Validation loss: 1.695.. \n",
      "Epoch 20/20.. Train loss: 3.753.. Validation loss: 1.78.. \n",
      "Epoch 20/20.. Train loss: 7.073.. Validation loss: 1.481.. \n",
      "Epoch 20/20.. Train loss: 1.988.. Validation loss: 1.547.. \n",
      "Epoch 20/20.. Train loss: 0.038.. Validation loss: 1.559.. \n",
      "Epoch 20/20.. Train loss: 1.565.. Validation loss: 1.627.. \n",
      "Epoch 20/20.. Train loss: 0.029.. Validation loss: 1.612.. \n",
      "Epoch 20/20.. Train loss: 0.413.. Validation loss: 1.576.. \n",
      "Epoch 20/20.. Train loss: 0.051.. Validation loss: 1.569.. \n",
      "Epoch 20/20.. Train loss: 0.258.. Validation loss: 1.584.. \n",
      "Epoch 20/20.. Train loss: 0.138.. Validation loss: 1.596.. \n",
      "Epoch 20/20.. Train loss: 0.013.. Validation loss: 1.6.. \n",
      "Epoch 20/20.. Train loss: 0.481.. Validation loss: 1.555.. \n",
      "Epoch 20/20.. Train loss: 0.148.. Validation loss: 1.566.. \n",
      "Epoch 20/20.. Train loss: 0.146.. Validation loss: 1.539.. \n",
      "Epoch 20/20.. Train loss: 2.776.. Validation loss: 1.569.. \n",
      "Epoch 20/20.. Train loss: 3.410.. Validation loss: 1.419.. \n",
      "Epoch 20/20.. Train loss: 0.050.. Validation loss: 1.427.. \n",
      "Epoch 20/20.. Train loss: 0.171.. Validation loss: 1.413.. \n",
      "Epoch 20/20.. Train loss: 0.290.. Validation loss: 1.436.. \n",
      "Epoch 20/20.. Train loss: 2.601.. Validation loss: 1.417.. \n",
      "Epoch 20/20.. Train loss: 0.126.. Validation loss: 1.428.. \n",
      "Epoch 20/20.. Train loss: 0.226.. Validation loss: 1.438.. \n",
      "Epoch 20/20.. Train loss: 0.002.. Validation loss: 1.439.. \n",
      "Epoch 20/20.. Train loss: 0.001.. Validation loss: 1.438.. \n",
      "Epoch 20/20.. Train loss: 0.008.. Validation loss: 1.435.. \n",
      "Epoch 20/20.. Train loss: 0.008.. Validation loss: 1.431.. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20.. Train loss: 0.007.. Validation loss: 1.433.. \n",
      "Epoch 20/20.. Train loss: 0.028.. Validation loss: 1.437.. \n",
      "Epoch 20/20.. Train loss: 0.353.. Validation loss: 1.417.. \n",
      "Epoch 20/20.. Train loss: 24.432.. Validation loss: 1.648.. \n",
      "Epoch 20/20.. Train loss: 0.798.. Validation loss: 1.569.. \n",
      "Epoch 20/20.. Train loss: 0.547.. Validation loss: 1.61.. \n",
      "Epoch 20/20.. Train loss: 0.593.. Validation loss: 1.565.. \n",
      "Epoch 20/20.. Train loss: 0.446.. Validation loss: 1.605.. \n",
      "Epoch 20/20.. Train loss: 1.671.. Validation loss: 1.704.. \n",
      "Epoch 20/20.. Train loss: 0.179.. Validation loss: 1.744.. \n",
      "Epoch 20/20.. Train loss: 0.137.. Validation loss: 1.722.. \n",
      "Epoch 20/20.. Train loss: 0.002.. Validation loss: 1.721.. \n",
      "Epoch 20/20.. Train loss: 0.632.. Validation loss: 1.667.. \n",
      "Epoch 20/20.. Train loss: 0.271.. Validation loss: 1.632.. \n",
      "Epoch 20/20.. Train loss: 7.064.. Validation loss: 1.42.. \n",
      "Epoch 20/20.. Train loss: 0.221.. Validation loss: 1.407.. \n",
      "Epoch 20/20.. Train loss: 1.400.. Validation loss: 1.435.. \n",
      "Epoch 20/20.. Train loss: 0.384.. Validation loss: 1.461.. \n",
      "Epoch 20/20.. Train loss: 0.436.. Validation loss: 1.492.. \n",
      "Epoch 20/20.. Train loss: 0.015.. Validation loss: 1.495.. \n",
      "Epoch 20/20.. Train loss: 2.853.. Validation loss: 1.598.. \n",
      "Epoch 20/20.. Train loss: 0.109.. Validation loss: 1.619.. \n",
      "Epoch 20/20.. Train loss: 1.256.. Validation loss: 1.689.. \n",
      "Epoch 20/20.. Train loss: 0.286.. Validation loss: 1.743.. \n",
      "Epoch 20/20.. Train loss: 0.802.. Validation loss: 1.81.. \n",
      "Epoch 20/20.. Train loss: 0.288.. Validation loss: 1.864.. \n",
      "Epoch 20/20.. Train loss: 0.002.. Validation loss: 1.868.. \n",
      "Epoch 20/20.. Train loss: 0.675.. Validation loss: 1.833.. \n",
      "Epoch 20/20.. Train loss: 1.624.. Validation loss: 1.954.. \n",
      "Epoch 20/20.. Train loss: 0.133.. Validation loss: 2.003.. \n",
      "Epoch 20/20.. Train loss: 2.282.. Validation loss: 1.878.. \n",
      "Epoch 20/20.. Train loss: 0.305.. Validation loss: 1.918.. \n",
      "Epoch 20/20.. Train loss: 0.156.. Validation loss: 1.971.. \n",
      "Epoch 20/20.. Train loss: 1.013.. Validation loss: 1.906.. \n",
      "Epoch 20/20.. Train loss: 0.567.. Validation loss: 1.854.. \n",
      "Epoch 20/20.. Train loss: 51.010.. Validation loss: 3.052.. \n",
      "Epoch 20/20.. Train loss: 2.470.. Validation loss: 2.779.. \n",
      "Epoch 20/20.. Train loss: 0.035.. Validation loss: 2.807.. \n",
      "Epoch 20/20.. Train loss: 2.794.. Validation loss: 2.495.. \n",
      "Epoch 20/20.. Train loss: 1.327.. Validation loss: 2.447.. \n",
      "Epoch 20/20.. Train loss: 0.695.. Validation loss: 2.549.. \n",
      "Epoch 20/20.. Train loss: 8.863.. Validation loss: 1.75.. \n",
      "Epoch 20/20.. Train loss: 0.918.. Validation loss: 1.84.. \n",
      "Epoch 20/20.. Train loss: 0.015.. Validation loss: 1.851.. \n",
      "Epoch 20/20.. Train loss: 0.432.. Validation loss: 1.765.. \n"
     ]
    }
   ],
   "source": [
    "train(model ,train_loader, val_loader, epochs, criterion, optimizer, \"model_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-jungle",
   "metadata": {},
   "source": [
    "### 11. Show some predictions and compare with ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "spatial-visiting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground-truth:  11.09000015258789  Predicted:  9.68852710723877\n",
      "ground-truth:  7.599999904632568  Predicted:  8.337696075439453\n",
      "ground-truth:  6.360000133514404  Predicted:  6.699223041534424\n",
      "ground-truth:  7.199999809265137  Predicted:  7.96565580368042\n",
      "ground-truth:  7.5  Predicted:  5.714795112609863\n",
      "ground-truth:  7.599999904632568  Predicted:  6.462888717651367\n",
      "ground-truth:  11.600000381469727  Predicted:  11.801243782043457\n",
      "ground-truth:  1.9700000286102295  Predicted:  2.662522315979004\n",
      "ground-truth:  1.600000023841858  Predicted:  2.18390154838562\n",
      "ground-truth:  5.880000114440918  Predicted:  6.4373040199279785\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"ground-truth: \",float(val_ds[i][1]), \" Predicted: \",float(model(val_ds[i][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-customer",
   "metadata": {},
   "source": [
    "### 12. Measure Pearson Corrrelation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "skilled-radiation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.        0.9358218]\n",
      " [0.9358218 1.       ]]\n",
      "[[1.         0.93812422]\n",
      " [0.93812422 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "y_val = val_ds.dataset.tensors[1].cpu().detach().numpy()\n",
    "\n",
    "pred_train = np.ones(95, dtype=np.float32)\n",
    "y_train = np.ones(95, dtype=np.float32)\n",
    "\n",
    "pred_val = np.ones(63, dtype=np.float32)\n",
    "y_val = np.ones(63, dtype=np.float32)\n",
    "\n",
    "count = 0\n",
    "for point in train_ds:\n",
    "    pred_train[count] = float(model(point[0]))\n",
    "    y_train[count] = float(point[1])\n",
    "    count+=1\n",
    "    \n",
    "count = 0\n",
    "for point in val_ds:\n",
    "    pred_val[count] = float(model(point[0]))\n",
    "    y_val[count] = float(point[1])\n",
    "    count+=1\n",
    "\n",
    "corr_train = np.corrcoef(y_train,pred_train, np.float64)\n",
    "print(corr_train)\n",
    "\n",
    "corr_val = np.corrcoef(y_val,pred_val, np.float64)\n",
    "print(corr_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-knife",
   "metadata": {},
   "source": [
    "### 13. Regularization\n",
    "Regularization is used in order to avoid overfitting, for that the SGD Optimizer aproach already deals with L2 Regularization which is the same as this model trained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cruise_env",
   "language": "python",
   "name": "cruise_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
